import asyncio
import logging
import hashlib
import json
import os
import sys
import re
import random
import requests
import time
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import feedparser
from PIL import Image, ImageDraw, ImageFont
from telegram import Bot
from telegram.error import TelegramError
from fake_useragent import UserAgent
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import io
import banned_organizations

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è - —Ç–µ–ø–µ—Ä—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –±–ª–æ–∫
if 'RAILWAY_ENVIRONMENT' in os.environ or 'RAILWAY_STATIC_URL' in os.environ:
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è Railway
    TOKEN = os.environ.get('TELEGRAM_TOKEN', "7445394461:AAGHiGYBiCwEg-tbchU9lOJmywv4CjcKuls")
    CHANNEL_ID = os.environ.get('TELEGRAM_CHANNEL', "@techno_met")
else:
    # –õ–æ–∫–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    TOKEN = "7445394461:AAGHiGYBiCwEg-tbchU9lOJmywv4CjcKuls"
    CHANNEL_ID = "@techno_met"

IXBT_RSS_URL = "https://www.ixbt.com/export/news.rss"
CHECK_INTERVAL = 1800  # 30 –º–∏–Ω—É—Ç

# –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –æ–±–ª–∞–∫–µ
def ensure_directories():
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
    directories = ['images', 'downloaded_images']
    for directory in directories:
        os.makedirs(directory, exist_ok=True)

# –í—ã–∑–æ–≤ –≤ –Ω–∞—á–∞–ª–µ
ensure_directories()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ httpx
logging.getLogger('httpx').setLevel(logging.WARNING)

class SmartNewsBot:
    def __init__(self):
        self.processed_news = set()
        self.load_processed_news()
        self.bot = Bot(token=TOKEN)
        self.ua = UserAgent()
        self.session = self.create_advanced_session()
        
    def create_advanced_session(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Å–µ—Å—Å–∏–∏ —Å –æ–±—Ö–æ–¥–æ–º –∑–∞—â–∏—Ç—ã"""
        session = requests.Session()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º retry —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"],
            backoff_factor=1
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        return session

    def check_banned_organizations(self, title, text):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        content = f"{title} {text}".lower()
        
        found_organizations = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        for org in banned_organizations.BANNED_ORGANIZATIONS:
            if org.lower() in content:
                found_organizations.append(org)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in banned_organizations.BANNED_KEYWORDS:
            if keyword in content:
                # –ò—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
                start = max(0, content.find(keyword) - 50)
                end = min(len(content), content.find(keyword) + len(keyword) + 50)
                context = content[start:end]
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–æ–∑–º–æ–∂–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                words = context.split()
                if len(words) > 2:
                    potential_org = ' '.join(words[:min(5, len(words))])
                    found_organizations.append(f"–∫–æ–Ω—Ç–µ–∫—Å—Ç: {potential_org}...")
        
        return found_organizations


    def load_processed_news(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists('processed_news.json'):
                with open('processed_news.json', 'r') as f:
                    data = json.load(f)
                    self.processed_news = set(data)
        except Exception as e:
            logger.error(f"Error loading processed news: {e}")
            self.processed_news = set()

    def save_processed_news(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Ñ–∞–π–ª"""
        try:
            with open('processed_news.json', 'w') as f:
                json.dump(list(self.processed_news), f)
        except Exception as e:
            logger.error(f"Error saving processed news: {e}")

    def get_news_hash(self, title, link):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ö—ç—à–∞ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏"""
        return hashlib.md5(f"{title}{link}".encode()).hexdigest()

    def detect_news_category(self, title, text):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ —Ö–µ—à—Ç–µ–≥–æ–≤"""
        title_lower = title.lower()
        text_lower = text.lower()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        full_text = f"{title_lower} {text_lower}"
        
        # –£—Ç–æ—á–Ω–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –≤–µ—Å–∞–º–∏ –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
        categories = {
            'ai': {
                'keywords': [
                    '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–Ω–µ–π—Ä–æ—Å–µ—Ç—å', '–Ω–µ–π—Ä–æ—Å–µ—Ç–∏', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', 
                    'ai', 'chatgpt', 'gpt', 'openai', 'deepmind', 'ml ', ' dl ', 'computer vision',
                    '–æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞', '–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π ai', 'stable diffusion', 
                    'midjourney', 'llm', 'large language model', '—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä', 'transformer',
                    'ai agent', 'deep learning', '–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'training', 'inference',
                    'ai ethics', '—ç—Ç–∏–∫–∞ –∏–∏', '—Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å', 'superintelligence', 'ai safety',
                    'data science', 'data mining', 'big data', '–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ',
                    '—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤', 'ai chip', 'tensorflow', 'pytorch', 'hugging face',
                    'chatbot', 'ai assistant', '–≥–æ–ª–æ—Å–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫', 'recommendation system',
                    'autonomous', 'ai generated', 'synthetic data', 'synthetic media', 'deepfake'
                ],
                'exclude': ['–∏–≥—Ä–∞', '–∏–≥—Ä–æ–≤–æ–π', 'gaming'],  # –ò—Å–∫–ª—é—á–µ–Ω–∏—è —á—Ç–æ–±—ã –Ω–µ –ø—É—Ç–∞—Ç—å —Å –∏–≥—Ä–∞–º–∏
                'weight': 0,
                'priority': 1
            },
            'space': {
                'keywords': [
                    '–∫–æ—Å–º–æ—Å', 'spacex', 'nasa', '–º–∞—Ä—Å', '–ª—É–Ω–∞', '—Å–ø—É—Ç–Ω–∏–∫', '–æ—Ä–±–∏—Ç–∞', '—Ä–∞–∫–µ—Ç–∞',
                    'starlink', 'roscosmos', 'esa', '–∫–æ—Å–º–æ–Ω–∞–≤—Ç', '–∞—Å—Ç—Ä–æ–Ω–∞–≤—Ç', '—Ç–µ–ª–µ—Å–∫–æ–ø',
                    'james webb', 'hubble', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è –∫–æ—Å–º–∏—á–µ—Å–∫–∞—è —Å—Ç–∞–Ω—Ü–∏—è', '–º–∫—Å', 'iss',
                    '–∑–∞–ø—É—Å–∫', '—Å—Ç–∞—Ä—Ç', '–ø–æ—Å–∞–¥–∫–∞', 'starship', 'falcon', 'dragon', '—Å–æ—é–∑',
                    '–≤–Ω–µ–∑–µ–º–Ω–∞—è –∂–∏–∑–Ω—å', '–∏–Ω–æ–ø–ª–∞–Ω–µ—Ç—è–Ω–µ', 'alien', '—ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç–∞', 'exoplanet',
                    '—á–µ—Ä–Ω–∞—è –¥—ã—Ä–∞', 'black hole', 'neutron star', '–Ω–µ–π—Ç—Ä–æ–Ω–Ω–∞—è –∑–≤–µ–∑–¥–∞',
                    '–≥–∞–ª–∞–∫—Ç–∏–∫–∞', 'galaxy', '–º–ª–µ—á–Ω—ã–π –ø—É—Ç—å', 'solar system', '—Å–æ–ª–Ω–µ—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞',
                    '–∞—Å—Ç–µ—Ä–æ–∏–¥', '–∫–æ–º–µ—Ç–∞', '–∫–æ—Å–º–∏—á–µ—Å–∫–∏–π –º—É—Å–æ—Ä', 'space debris', 'orbital',
                    '–∫–æ—Å–º–æ–¥—Ä–æ–º', 'baikonur', 'artemis', 'apollo', '–∫–æ—Å–º–∏—á–µ—Å–∫–∏–π —Ç—É—Ä–∏–∑–º'
                ],
                'weight': 0,
                'priority': 1
            },
            'gadgets': {
                'keywords': [
                    '—Å–º–∞—Ä—Ç—Ñ–æ–Ω', '—Ç–µ–ª–µ—Ñ–æ–Ω', 'iphone', 'android', '–ø–ª–∞–Ω—à–µ—Ç', '–Ω–æ—É—Ç–±—É–∫', '–≥–∞–¥–∂–µ—Ç',
                    '—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ', 'apple', 'samsung', 'xiaomi', 'huawei', 'google pixel',
                    'oneplus', 'oppo', 'vivo', 'realme', '—É–º–Ω—ã–µ —á–∞—Å—ã', 'smartwatch', 
                    'apple watch', 'samsung galaxy watch', '—Ñ–∏—Ç–Ω–µ—Å-–±—Ä–∞—Å–ª–µ—Ç', 'fitness tracker',
                    'xiaomi mi band', '–Ω–∞—É—à–Ω–∏–∫–∏', 'airpods', 'galaxy buds', 'wireless earbuds',
                    'bluetooth –Ω–∞—É—à–Ω–∏–∫–∏', '–∫–æ–ª–æ–Ω–∫–∞', 'smart speaker', 'amazon echo', 
                    'google home', 'apple homepod', '—É–º–Ω—ã–π –¥–æ–º', 'smart home', 'iot',
                    '—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∫–Ω–∏–≥–∞', 'e-book', 'kindle', 'pocketbook', 'onyx boox',
                    'ipad', 'surface', 'macbook', 'dell', 'lenovo', 'hp', 'asus',
                    '—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä', '2-–≤-1', '–≥–∏–±—Ä–∏–¥', 'foldable', '—Å–∫–ª–∞–¥—ã–≤–∞—é—â–∏–π—Å—è'
                ],
                'weight': 0,
                'priority': 2
            },
            'tv': {
                'keywords': [
                    '—Ç–µ–ª–µ–≤–∏–∑–æ—Ä', '—Ç–≤', 'oled', 'qled', '4k', '8k', '—ç–∫—Ä–∞–Ω', '–¥–∏—Å–ø–ª–µ–π',
                    'ultra hd', 'full hd', 'hd', '—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ', 'hdr', 'dolby vision',
                    'hdr10', 'hdr10+', 'hlg', 'smart tv', 'android tv', 'webos', 'tizen',
                    'roku tv', '—á–∞—Å—Ç–æ—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è', 'refresh rate', '60hz', '120hz',
                    '240hz', '–ø–æ–¥—Å–≤–µ—Ç–∫–∞', 'local dimming', '–∏–∑–æ–≥–Ω—É—Ç—ã–π —ç–∫—Ä–∞–Ω', 'curved',
                    '—Ç–æ–ª—â–∏–Ω–∞', 'thin', '–±–µ–∑—Ä–∞–º–æ—á–Ω—ã–π', 'bezelless', '—Ü–≤–µ—Ç–æ–ø–µ—Ä–µ–¥–∞—á–∞',
                    'color gamut', 'dci-p3', 'rec.2020', '—è—Ä–∫–æ—Å—Ç—å', 'nit', '–∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å',
                    '–ø–∞–Ω–µ–ª—å', 'va', 'ips', 'quantum dot', '–∫–≤–∞–Ω—Ç–æ–≤—ã–µ —Ç–æ—á–∫–∏', 'laser tv',
                    '–ø—Ä–æ–µ–∫—Ç–æ—Ä', 'home theater', '–¥–æ–º–∞—à–Ω–∏–π –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä', '–∑–≤—É–∫', 'audio',
                    'dolby atmos', 'dts', 'soundbar', '—Å–∞—É–Ω–¥–±–∞—Ä', 'lg', 'samsung', 'sony'
                ],
                'weight': 0,
                'priority': 2
            },
            'auto': {
                'keywords': [
                    '–∞–≤—Ç–æ', '–º–∞—à–∏–Ω–∞', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', 'tesla', '—ç–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª—å', 'toyota',
                    'hyundai', '–º–∞—Ä–∫–∞', '–º–æ–¥–µ–ª—å', 'bmw', 'mercedes', 'audi', 'volkswagen',
                    'ford', 'chevrolet', 'nissan', 'honda', 'kia', 'volvo', 'renault',
                    'peugeot', '–¥–≤–∏–≥–∞—Ç–µ–ª—å', 'engine', '–±–µ–Ω–∑–∏–Ω–æ–≤—ã–π', '–¥–∏–∑–µ–ª—å–Ω—ã–π', '–≥–∏–±—Ä–∏–¥',
                    'hybrid', 'plug-in hybrid', 'phev', '—Ç—Ä–∞–Ω—Å–º–∏—Å—Å–∏—è', '–∫–æ—Ä–æ–±–∫–∞ –ø–µ—Ä–µ–¥–∞—á',
                    '–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è', '–º–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∞—è', '—Ä–æ–±–æ—Ç', '–≤–∞—Ä–∏–∞—Ç–æ—Ä', 'dsg', '–ø—Ä–∏–≤–æ–¥',
                    '–ø–µ—Ä–µ–¥–Ω–∏–π', '–∑–∞–¥–Ω–∏–π', '–ø–æ–ª–Ω—ã–π', 'awd', '4wd', '–∏–Ω—Ç–µ—Ä—å–µ—Ä', '—Å–∞–ª–æ–Ω',
                    '–º—É–ª—å—Ç–∏–º–µ–¥–∏—è', 'infotainment', 'apple carplay', 'android auto',
                    '–∫—Ä—É–∏–∑-–∫–æ–Ω—Ç—Ä–æ–ª—å', '–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∫—Ä—É–∏–∑-–∫–æ–Ω—Ç—Ä–æ–ª—å', '–∞–≤—Ç–æ–ø–∏–ª–æ—Ç', 'autopilot',
                    '–≤–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã', 'lane keeping', '–∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –≤–æ–∂–¥–µ–Ω–∏–µ',
                    'self-driving', '–±–µ—Å–ø–∏–ª–æ—Ç–Ω–∏–∫', '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', 'ncap', '–ø–æ–¥—É—à–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏',
                    'abs', 'esp', '–ø–∞—Ä–∫–æ–≤–∫–∞', '–∫–∞–º–µ—Ä–∞ –∑–∞–¥–Ω–µ–≥–æ –≤–∏–¥–∞', '–¥–∞—Ç—á–∏–∫–∏ –ø–∞—Ä–∫–æ–≤–∫–∏'
                ],
                'weight': 0,
                'priority': 2
            },
            'games': {
                'keywords': [
                    '–∏–≥—Ä–∞', '–∏–≥—Ä–æ–≤–æ–π', 'gaming', 'playstation', 'xbox', 'nintendo', 'steam',
                    '–∫–æ–Ω—Å–æ–ª—å', 'pc', '–ø–∫', '–≤–∏–¥–µ–æ–∏–≥—Ä–∞', 'ps5', 'ps4', 'xbox series x',
                    'xbox series s', 'nintendo switch', 'playstation plus', 'xbox game pass',
                    'game pass ultimate', 'ea play', 'ubisoft connect', 'epic games store',
                    'gog', 'release', '—Ä–µ–ª–∏–∑', '–∞–Ω–æ–Ω—Å', '—Ç—Ä–µ–π–ª–µ—Ä', 'gameplay', '–≥–µ–π–º–ø–ª–µ–π',
                    '—Å—é–∂–µ—Ç', 'story', '–≥—Ä–∞—Ñ–∏–∫–∞', 'graphics', 'fps', 'rpg', 'mmo', 'mmorpg',
                    '—à—É—Ç–µ—Ä', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', '—ç–∫—à–µ–Ω', '–ø—Ä–∏–∫–ª—é—á–µ–Ω–∏–µ', '–∏–Ω–¥–∏', '–∫–∏–±–µ—Ä—Å–ø–æ—Ä—Ç',
                    'esports', 'twitch', 'youtube gaming', '—Å—Ç—Ä–∏–º', 'stream', 'dlc',
                    '–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ', 'addon', 'expansion', '–ø–∞—Ç—á', 'update', '–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ',
                    'pre-order', '–ø—Ä–µ–¥–∑–∞–∫–∞–∑', '—Ä–µ–π—Ç–∏–Ω–≥', 'metacritic', 'openworld',
                    '–æ—Ç–∫—Ä—ã—Ç—ã–π –º–∏—Ä', 'multiplayer', '–º—É–ª—å—Ç–∏–ø–ª–µ–µ—Ä', 'co-op', '–∫–æ–æ–ø–µ—Ä–∞—Ç–∏–≤',
                    'pvp', 'pve', 'vr –∏–≥—Ä—ã', 'ar –∏–≥—Ä—ã', 'mobile game', '–º–æ–±–∏–ª—å–Ω–∞—è –∏–≥—Ä–∞',
                    'gacha', 'roblox', 'minecraft', 'fortnite', 'call of duty', 'warzone'
                ],
                'weight': 0,
                'priority': 2
            },
            'science': {
                'keywords': [
                    '–Ω–∞—É–∫–∞', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '—É—á–µ–Ω—ã–µ', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç', '–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è',
                    'scientific', 'discovery', 'study', 'research', '—Ñ–∏–∑–∏–∫–∞', 'physics',
                    '–∫–≤–∞–Ω—Ç–æ–≤–∞—è —Ñ–∏–∑–∏–∫–∞', 'quantum', '—Ç–µ–æ—Ä–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏', 'astrophysics',
                    '–∞—Å—Ç—Ä–æ—Ñ–∏–∑–∏–∫–∞', '–∫–æ—Å–º–æ–ª–æ–≥–∏—è', '—Ö–∏–º–∏—è', 'chemistry', '–±–∏–æ–ª–æ–≥–∏—è', 'biology',
                    '–≥–µ–Ω–µ—Ç–∏–∫–∞', 'genetics', 'dna', '—Ä–Ω–∫', 'rna', '–≥–µ–Ω–æ–º', 'genome', 'crispr',
                    '–º–µ–¥–∏—Ü–∏–Ω–∞', 'medicine', '–≤–∏—Ä—É—Å', 'vaccine', '–≤–∞–∫—Ü–∏–Ω–∞', '–∏–º–º—É–Ω–∏—Ç–µ—Ç',
                    'immunity', '–∞–Ω—Ç–∏—Ç–µ–ª–æ', 'antibody', '–∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—ã—Ç–∞–Ω–∏–µ', '–∞—Ä—Ö–µ–æ–ª–æ–≥–∏—è',
                    'archaeology', '–∞–Ω—Ç—Ä–æ–ø–æ–ª–æ–≥–∏—è', 'anthropology', '–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è', 'psychology',
                    '–Ω–µ–π—Ä–æ–±–∏–æ–ª–æ–≥–∏—è', 'neuroscience', '–º–æ–∑–≥', '–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞', 'mathematics',
                    '—Ç–µ–æ—Ä–µ–º–∞', '–≥–∏–ø–æ—Ç–µ–∑–∞', '–∞–ª–≥–æ—Ä–∏—Ç–º', '–º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤–µ–¥–µ–Ω–∏–µ', 'nanotechnology',
                    '–Ω–∞–Ω–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞', 'robotics', '–±–∏–æ–Ω–∏–∫–∞', '–±–∏–æ–∏–Ω–∂–µ–Ω–µ—Ä–∏—è',
                    '—ç–∫–æ–ª–æ–≥–∏—è', 'ecology', 'climate change', '–∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–ª–∏–º–∞—Ç–∞'
                ],
                'weight': 0,
                'priority': 1
            },
            'internet': {
                'keywords': [
                    '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '–±—Ä–∞—É–∑–µ—Ä', '—Å–æ—Ü—Å–µ—Ç—å', '—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è —Å–µ—Ç—å', 'facebook', 'instagram',
                    'tiktok', 'twitter', 'x ', 'youtube', 'linkedin', 'vk', 'telegram',
                    'whatsapp', 'wechat', 'signal', 'discord', 'reddit', 'pinterest', 'snapchat',
                    '–ø—Ä–æ–≤–∞–π–¥–µ—Ä', 'isp', '—Å–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞', 'broadband', '–æ–ø—Ç–æ–≤–æ–ª–æ–∫–Ω–æ',
                    'fiber', '5g', 'wi-fi', 'wi-fi 6', 'wi-fi 7', '—Ä–æ—É—Ç–µ—Ä', '–º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä',
                    'mesh-—Å–∏—Å—Ç–µ–º–∞', '—Ç—Ä–∞—Ñ–∏–∫', 'data', 'vpn', 'proxy', 'tor', '–∞–Ω–æ–Ω–∏–º–Ω–æ—Å—Ç—å',
                    'privacy', '–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å', 'cookies', 'tracking', '–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Å–∞–π—Ç–æ–≤',
                    'net neutrality', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –≤–µ—â–µ–π', 'iot', '—É–º–Ω—ã–π –¥–æ–º', '—Å–º–∞—Ä—Ç-–≥–æ—Ä–æ–¥',
                    'web 3.0', '–º–µ—Ç–∞–≤—Å–µ–ª–µ–Ω–Ω–∞—è', 'metaverse', 'seo', 'sem', '–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è —Ä–µ–∫–ª–∞–º–∞',
                    '—Å–∞–π—Ç', '–≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '—Ö–æ—Å—Ç–∏–Ω–≥', '–¥–æ–º–µ–Ω'
                ],
                'weight': 0,
                'priority': 2
            },
            'software': {
                'keywords': [
                    '–ø—Ä–æ–≥—Ä–∞–º–º–∞', '—Å–æ—Ñ—Ç', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ', 'windows', 'linux', 'macos',
                    'ios', 'android', 'api', '–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å', 'ui', 'ux', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', 'development',
                    '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', 'coding', 'agile', 'scrum', 'devops', 'github', 'gitlab',
                    'bitbucket', 'ide', 'visual studio', 'vs code', 'jetbrains', 'intellij',
                    'pycharm', '–∫–æ–º–ø–∏–ª—è—Ç–æ—Ä', '–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä', '—Å–∫—Ä–∏–ø—Ç', 'script', 'open source',
                    '–∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥', 'source code', '–ª–∏—Ü–µ–Ω–∑–∏—è', 'license', 'gpl', 'mit', 'freeware',
                    'shareware', 'proprietary', '–±–∞–≥', '–æ—à–∏–±–∫–∞', 'debugging', '–æ—Ç–ª–∞–¥–∫–∞',
                    '—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', 'qa', 'quality assurance', 'unit test', '–ø–∞—Ç—á', 'hotfix',
                    '—Ä–µ–ª–∏–∑', '–≤–µ—Ä—Å–∏—è', 'version', 'changelog', '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', 'readme'
                ],
                'weight': 0,
                'priority': 2
            },
            'hardware': {
                'keywords': [
                    '–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä', '–≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞', '–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å', 'ssd', '–∂–µ—Å—Ç–∫–∏–π –¥–∏—Å–∫',
                    '–º–∞—Ç–µ—Ä–∏–Ω—Å–∫–∞—è –ø–ª–∞—Ç–∞', 'cpu', 'gpu', 'ram', 'hdd', 'intel', 'amd', 'nvidia',
                    'qualcomm', 'apple silicon', 'm1', 'm2', 'ryzen', 'core i3', 'core i5',
                    'core i7', 'core i9', 'radeon', 'geforce', 'rtx', 'gtx', 'dlss', 'ray tracing',
                    '—Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –ª—É—á–µ–π', '—Ç–∞–∫—Ç–æ–≤—è —á–∞—Å—Ç–æ—Ç–∞', 'clock speed', '—Ä–∞–∑–≥–æ–Ω', 'overclocking',
                    '–æ—Ö–ª–∞–∂–¥–µ–Ω–∏–µ', 'cooling', '–∫—É–ª–µ—Ä', '—Ä–∞–¥–∏–∞—Ç–æ—Ä', '—Ç–µ—Ä–º–æ–ø–∞—Å—Ç–∞', '–≤–æ–¥—è–Ω–æ–µ –æ—Ö–ª–∞–∂–¥–µ–Ω–∏–µ',
                    'aio', 'liquid cooling', 'thermal throttling', '–±–ª–æ–∫ –ø–∏—Ç–∞–Ω–∏—è', 'psu', '–º–æ—â–Ω–æ—Å—Ç—å',
                    'efficiency', '80 plus', 'bronze', 'gold', 'platinum', '–∫–æ—Ä–ø—É—Å', 'case',
                    'atx', 'mini-itx', 'micro-atx', '—Ñ–æ—Ä–º-—Ñ–∞–∫—Ç–æ—Ä', '—Å–±–æ—Ä–∫–∞ –ø–∫', 'pc build',
                    '–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ç–æ—Ä', 'upgrade', '–∞–ø–≥—Ä–µ–π–¥', '–ø–µ—Ä–∏—Ñ–µ—Ä–∏—è', '–∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞', '–º—ã—à—å',
                    '–º–æ–Ω–∏—Ç–æ—Ä', '–ø—Ä–∏–Ω—Ç–µ—Ä', '—Å–∫–∞–Ω–µ—Ä', '–≤–µ–±-–∫–∞–º–µ—Ä–∞', '–º–∏–∫—Ä–æ—Ñ–æ–Ω'
                ],
                'weight': 0,
                'priority': 2
            },
            'security': {
                'keywords': [
                    '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '–≤–∏—Ä—É—Å', '—Ö–∞–∫–µ—Ä', '–∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '—à–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ', '–ø–∞—Ä–æ–ª—å',
                    '–∑–∞—â–∏—Ç–∞', 'malware', 'ransomware', 'trojan', 'spyware', '–∞–Ω—Ç–∏–≤–∏—Ä—É—Å', 'antivirus',
                    'kaspersky', 'eset', 'norton', 'mcafee', 'bitdefender', 'avast', '–±—Ä–∞–Ω–¥–º–∞—É—ç—Ä',
                    'firewall', '—Å–µ—Ç–µ–≤–æ–π —ç–∫—Ä–∞–Ω', 'ids', 'ips', '–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—Ç–æ—Ä–∂–µ–Ω–∏–π', 'prevention',
                    '–∞—Ç–∞–∫–∞', 'attack', 'ddos', '—Ñ–∏—à–∏–Ω–≥', 'phishing', '—Å–ø–∞–º', 'social engineering',
                    '—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è', '—É—è–∑–≤–∏–º–æ—Å—Ç—å –Ω—É–ª–µ–≤–æ–≥–æ –¥–Ω—è', 'zero-day', 'exploit',
                    '–ø–∞—Ç—á', 'update', '–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏', '–∫—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏—è', 'cryptography',
                    'aes', 'rsa', 'ssl', 'tls', 'https', '—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç', 'certificate', 'pki',
                    'data breach', '—É—Ç–µ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö', 'leak', 'information security', '–±–∏–æ–º–µ—Ç—Ä–∏—è',
                    'biometrics', '–æ—Ç–ø–µ—á–∞—Ç–æ–∫', 'face id', '—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–∏—Ü–∞', '–¥–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è',
                    '2fa', 'mfa', '–º–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω–∞—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è', '–∫–ª—é—á –¥–æ—Å—Ç—É–ø–∞', 'access key'
                ],
                'weight': 0,
                'priority': 2
            },
            'business': {
                'keywords': [
                    '–∫–æ–º–ø–∞–Ω–∏—è', '–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è', '—Å—Ç–∞—Ä—Ç–∞–ø', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '—Ä—ã–Ω–æ–∫', '–±–∏–∑–Ω–µ—Å',
                    '–ø—Ä–∏–±—ã–ª—å', '—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å', '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', 'ceo', 'cfo', 'cto', '–∞–∫—Ü–∏–∏',
                    'stock', '—Ñ–æ–Ω–¥–æ–≤—ã–π —Ä—ã–Ω–æ–∫', 'stock market', 'nasdaq', 'nyse', 'moex',
                    '–¥–∏–≤–∏–¥–µ–Ω–¥—ã', '–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è', 'market cap', '–≤–µ–Ω—á—É—Ä–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª', 'venture capital',
                    'vc', 'angel investor', '–±–∏–∑–Ω–µ—Å-–∞–Ω–≥–µ–ª', '–ø–æ—Å–µ–≤–Ω—ã–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', 'seed funding',
                    'series a', 'series b', 'ipo', 'spac', 'exit', 'merger', 'acquisition',
                    '—Å–ª–∏—è–Ω–∏—è –∏ –ø–æ–≥–ª–æ—â–µ–Ω–∏—è', 'm&a', 'due diligence', '—é—Ä–∏–¥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞',
                    '–±–∏–∑–Ω–µ—Å-–ø–ª–∞–Ω', 'pitch', '–ø–∏—Ç—á-–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è', '–∫–æ–≤–æ—Ä–∫–∏–Ω–≥', 'incubator', '–∏–Ω–∫—É–±–∞—Ç–æ—Ä',
                    '–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä', 'accelerator', 'y combinator', '–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç', 'management',
                    'hr', 'human resources', '—Ä–µ–∫—Ä—É—Ç–∏–Ω–≥', 'onboarding', '–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞—è –∫—É–ª—å—Ç—É—Ä–∞',
                    'remote work', '—É–¥–∞–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞', '–ø—Ä–æ–¥—É–∫—Ç', 'product', 'product manager',
                    'pm', 'project manager', '–ø—Ä–æ–µ–∫—Ç', 'agile', 'scrum', 'kanban', 'kpi', '–º–µ—Ç—Ä–∏–∫–∏'
                ],
                'weight': 0,
                'priority': 2
            },
            'crypto': {
                'keywords': [
                    '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞', '–±–∏—Ç–∫–æ–∏–Ω', 'bitcoin', 'btc', '—ç—Ñ–∏—Ä–∏—É–º', 'ethereum', 'eth',
                    '–±–ª–æ–∫—á–µ–π–Ω', 'blockchain', '–∞–ª—å—Ç–∫–æ–∏–Ω', 'altcoin', '–º–∞–π–Ω–∏–Ω–≥', 'mining',
                    '—Å—Ç–µ–π–∫–∏–Ω–≥', 'staking', 'defi', 'decentralized finance', 'nft', 'non-fungible token',
                    '—Ç–æ–∫–µ–Ω', 'token', 'coinbase', 'binance', 'bybit', 'kucoin', '–∫–æ—à–µ–ª–µ–∫', 'wallet',
                    'hardware wallet', 'ledger', 'trezor', '–º–µ—Ç–∞–º–æ—Ä—Å–∫', 'metamask', '—Å–º–∞—Ä—Ç-–∫–æ–Ω—Ç—Ä–∞–∫—Ç',
                    'smart contract', 'solidity', 'rust', 'web3', '–¥–∞–æ', 'dao', 'decentralized autonomous organization',
                    'ico', 'ieo', 'ido', 'initial coin offering', 'airdrop', '–∞–∏—Ä–¥—Ä–æ–ø', '–≥–∞–∑', 'gas fee',
                    '–∫–æ–º–∏—Å—Å–∏—è —Å–µ—Ç–∏', '—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è', '—Ö–∞–ª–≤–∏–Ω–≥', 'halving', '–±—ã–∫–∏', '–º–µ–¥–≤–µ–¥–∏',
                    'bull market', 'bear market', '–≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å', 'volatility', 'stablecoin',
                    '—Å—Ç–µ–π–±–ª–∫–æ–∏–Ω', 'usdt', 'usdc', 'dai', 'tether', '—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –±–∏—Ä–∂–∞', 'dex',
                    'decentralized exchange', 'uniswap', 'pancakeswap', '—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ', 'regulation'
                ],
                'weight': 0,
                'priority': 2
            },
            'health': {
                'keywords': [
                    '–∑–¥–æ—Ä–æ–≤—å–µ', '–º–µ–¥–∏—Ü–∏–Ω–∞', '–≤—Ä–∞—á', '–±–æ–ª—å–Ω–∏—Ü–∞', '–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞', '–ª–µ—á–µ–Ω–∏–µ',
                    '–∑–¥–æ—Ä–æ–≤—ã–π –æ–±—Ä–∞–∑ –∂–∏–∑–Ω–∏', '–∑–æ–∂', '–∑–¥–æ—Ä–æ–≤–æ–µ –ø–∏—Ç–∞–Ω–∏–µ', '–¥–∏–µ—Ç–∞', '—Ñ–∏—Ç–Ω–µ—Å',
                    '—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞', '—Å–ø–æ—Ä—Ç', '–±–µ–≥', '–π–æ–≥–∞', '–ø–ª–∞–≤–∞–Ω–∏–µ', '–∫–∞—Ä–¥–∏–æ', '—Å–∏–ª–æ–≤–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞',
                    '—Ä–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è', 'recovery', '–≤–∏—Ç–∞–º–∏–Ω—ã', '–±–∏–æ–¥–æ–±–∞–≤–∫–∏', 'supplements', '–∏–º–º—É–Ω–∏—Ç–µ—Ç',
                    '—Å–æ–Ω', 'sleep', '–ø—Å–∏—Ö–∏—á–µ—Å–∫–æ–µ –∑–¥–æ—Ä–æ–≤—å–µ', 'mental health', '—Å—Ç—Ä–µ—Å—Å', '—Ç—Ä–µ–≤–æ–∂–Ω–æ—Å—Ç—å',
                    'depression', '–¥–µ–ø—Ä–µ—Å—Å–∏—è', 'therapy', '—Ç–µ—Ä–∞–ø–∏—è', 'covid-19', '–∫–æ—Ä–æ–Ω–∞–≤–∏—Ä—É—Å',
                    '–ø–∞–Ω–¥–µ–º–∏—è', '–≤–∞–∫—Ü–∏–Ω–∞', '–≤–∞–∫—Ü–∏–Ω–∞—Ü–∏—è', 'booster', '—Ç–µ—Å—Ç', 'pcr', '–∞–Ω—Ç–∏–≥–µ–Ω',
                    '–∞–Ω—Ç–∏—Ç–µ–ª–æ', '–≥–µ–Ω—ã', '–≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç', 'dna test', '–¥–æ–ª–≥–æ–ª–µ—Ç–∏–µ', 'longevity',
                    'anti-age', 'anti-aging', '–±–∏–æ—Ö–∞–∫–µ—Ä', 'biohacking', '–≥–º–æ', 'gmo', '–æ—Ä–≥–∞–Ω–∏–∫',
                    'organic', 'superfood', '—Å—É–ø–µ—Ä—Ñ—É–¥', '–¥–µ—Ç–æ–∫—Å', '–≤–µ–≥–∞–Ω', '–≤–µ–≥–µ—Ç–∞—Ä–∏–∞–Ω–µ—Ü', '—Ö–∏—Ä—É—Ä–≥–∏—è',
                    'surgery', '–º–∏–∫—Ä–æ—Ö–∏—Ä—É—Ä–≥–∏—è', '—Ä–æ–±–æ—Ç-—Ö–∏—Ä—É—Ä–≥', 'da vinci', '—Ç–µ–ª–µ–º–µ–¥–∏—Ü–∏–Ω–∞', 'telehealth'
                ],
                'weight': 0,
                'priority': 2
            }
        }
        
        # –°—á–∏—Ç–∞–µ–º –≤–µ—Å –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
        for category, data in categories.items():
            weight = 0
            
            for keyword in data['keywords']:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –≥—Ä–∞–Ω–∏—Ü —Å–ª–æ–≤
                pattern = r'\b' + re.escape(keyword) + r'\b'
                
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏–º–µ–µ—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å (5 –±–∞–ª–ª–æ–≤)
                if re.search(pattern, title_lower):
                    weight += 5
                # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–º–µ–µ—Ç –º–µ–Ω—å—à–∏–π –≤–µ—Å (2 –±–∞–ª–ª–∞)
                elif re.search(pattern, full_text):
                    weight += 2
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è (—Å–Ω–∏–∂–∞–µ–º –≤–µ—Å –µ—Å–ª–∏ –µ—Å—Ç—å –∏—Å–∫–ª—é—á–∞—é—â–∏–µ —Å–ª–æ–≤–∞)
            if 'exclude' in data:
                for exclude_word in data['exclude']:
                    if exclude_word in full_text:
                        weight = max(0, weight - 3)  # –°—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞–µ–º –≤–µ—Å
            
            data['weight'] = weight
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –≤–µ—Å—É –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        def category_sort_key(item):
            category, data = item
            return (data['weight'], data.get('priority', 0))
        
        sorted_categories = sorted(categories.items(), key=category_sort_key, reverse=True)
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≤–µ—Å–æ–º > 3 –∏ –º–∞–∫—Å–∏–º—É–º 3 —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
        found_categories = [cat for cat, data in sorted_categories if data['weight'] > 3][:3]
        
        # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–±—É–µ–º —Å–Ω–∏–∑–∏—Ç—å –ø–æ—Ä–æ–≥
        if not found_categories:
            found_categories = [cat for cat, data in sorted_categories if data['weight'] > 1][:2]
        
        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–µ —Ç–µ–≥–∏
        if not found_categories:
            logger.info("‚ÑπÔ∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –æ–±—â–∏–µ —Ç–µ–≥–∏")
            return ['technology', 'news']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ —Ç–µ–≥–∏
        result_categories = found_categories + ['technology', 'news']
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        logger.info(f"üè∑Ô∏è –û–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {result_categories}")
        for cat in found_categories:
            logger.info(f"   - {cat}: –≤–µ—Å {categories[cat]['weight']}")
        
        return result_categories

    def get_hashtags_for_category(self, categories):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Ö–µ—à—Ç–µ–≥–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        category_hashtags = {
            'ai': ['–ò–ò', '–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–ù–µ–π—Ä–æ—Å–µ—Ç–∏', 'AI'],
            'space': ['–ö–æ—Å–º–æ—Å', 'SpaceX', 'NASA', '–ê—Å—Ç—Ä–æ–Ω–æ–º–∏—è'],
            'gadgets': ['–ì–∞–¥–∂–µ—Ç—ã', '–¢–µ—Ö–Ω–∏–∫–∞', '–≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞', '–£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'],
            'tv': ['–¢–µ–ª–µ–≤–∏–∑–æ—Ä—ã', '–¢–í', '–î–∏—Å–ø–ª–µ–∏', '4K'],
            'auto': ['–ê–≤—Ç–æ', '–ê–≤—Ç–æ–º–æ–±–∏–ª–∏', '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '–≠–ª–µ–∫—Ç—Ä–æ–º–æ–±–∏–ª–∏'],
            'games': ['–ò–≥—Ä—ã', '–ì–µ–π–º–∏–Ω–≥', '–ò–≥—Ä–æ–≤—ã–µ–ù–æ–≤–æ—Å—Ç–∏', '–ö–∏–±–µ—Ä—Å–ø–æ—Ä—Ç'],
            'science': ['–ù–∞—É–∫–∞', '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è', '–û—Ç–∫—Ä—ã—Ç–∏—è', '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏'],
            'internet': ['–ò–Ω—Ç–µ—Ä–Ω–µ—Ç', '–û–Ω–ª–∞–π–Ω', '–°–æ—Ü—Å–µ—Ç–∏', '–¶–∏—Ñ—Ä–æ–≤–∏–∑–∞—Ü–∏—è'],
            'software': ['–°–æ—Ñ—Ç', '–ü—Ä–æ–≥—Ä–∞–º–º—ã', '–ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è', '–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞'],
            'hardware': ['–ñ–µ–ª–µ–∑–æ', '–ö–æ–º–ø—å—é—Ç–µ—Ä—ã', '–ö–æ–º–ø–ª–µ–∫—Ç—É—é—â–∏–µ', '–ê–ø–≥—Ä–µ–π–¥'],
            'security': ['–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '–ö–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '–ó–∞—â–∏—Ç–∞–î–∞–Ω–Ω—ã—Ö', '–ê–Ω—Ç–∏–≤–∏—Ä—É—Å'],
            'business': ['–ë–∏–∑–Ω–µ—Å', '–°—Ç–∞—Ä—Ç–∞–ø—ã', '–ò–Ω–Ω–æ–≤–∞—Ü–∏–∏', '–¢–µ—Ö–Ω–æ–±–∏–∑–Ω–µ—Å'],
            'crypto': ['–ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞', '–ë–ª–æ–∫—á–µ–π–Ω', 'NFT', '–ë–∏—Ç–∫–æ–∏–Ω'],
            'health': ['–ó–¥–æ—Ä–æ–≤—å–µ', '–ú–µ–¥–∏—Ü–∏–Ω–∞', '–ë–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–ó–û–ñ'],
            'technology': ['–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', 'IT', '–ò–Ω–Ω–æ–≤–∞—Ü–∏–∏', '–¢–µ—Ö–Ω–æ–ù–æ–≤–æ—Å—Ç–∏'],
            'news': ['–ù–æ–≤–æ—Å—Ç–∏', '–°–≤–µ–∂–∏–µ–ù–æ–≤–æ—Å—Ç–∏', '–û–±–∑–æ—Ä']
        }
        
        hashtags = []
        used_hashtags = set()
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ö–µ—à—Ç–µ–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—É–∂–µ —Å #)
        for category in categories:
            if category in category_hashtags:
                for hashtag in category_hashtags[category]:
                    hashtag_with_hash = f"#{hashtag}"
                    if hashtag_with_hash not in used_hashtags:
                        hashtags.append(hashtag_with_hash)
                        used_hashtags.add(hashtag_with_hash)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∞–∫—Å–∏–º—É–º 5 —Ö–µ—à—Ç–µ–≥–æ–≤
        unique_hashtags = list(set(hashtags))[:5]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        unique_hashtags.sort()
        
        logger.info(f"üè∑Ô∏è –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã —Ö–µ—à—Ç–µ–≥–∏: {unique_hashtags}")
        return unique_hashtags

    async def fetch_news(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å IXBT"""
        try:
            headers = {
                'User-Agent': self.ua.random,
                'Accept': 'application/rss+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            }
            
            response = self.session.get(IXBT_RSS_URL, headers=headers, timeout=20)
            response.raise_for_status()
            
            feed = feedparser.parse(response.content)
            news_list = []
            
            for entry in feed.entries[:5]:
                news_hash = self.get_news_hash(entry.title, entry.link)
                
                if news_hash not in self.processed_news:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    await asyncio.sleep(random.uniform(2, 5))
                    
                    full_text, image_url = await self.get_full_article_text_and_image(entry.link)
                    
                    if len(full_text) > 150:
                        news_item = {
                            'title': entry.title,
                            'link': entry.link,
                            'summary': entry.summary,
                            'full_text': full_text,
                            'image_url': image_url,
                            'hash': news_hash,
                            'published': entry.published
                        }
                        news_list.append(news_item)
                        logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {entry.title}")
                        if image_url:
                            logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏: {image_url}")
                        else:
                            logger.warning("‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏")
                    else:
                        logger.warning(f"‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç: {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return news_list
        except Exception as e:
            logger.error(f"Error fetching news: {e}")
            return []

    async def get_full_article_text_and_image(self, url):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Å—Ç–∞—Ç—å–∏"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏ —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            methods = [
                self.method_smart_request,
                self.method_simple_request,
            ]
            
            best_result = None
            all_found_images = []  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–ï –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
            for method in methods:
                try:
                    full_text, image_url = await method(url)
                    logger.info(f"üîÑ –ú–µ—Ç–æ–¥ {method.__name__}: —Ç–µ–∫—Å—Ç={len(full_text) if full_text else 0}, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ={image_url}")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    if image_url and image_url not in all_found_images:
                        all_found_images.append(image_url)
                        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ –º–µ—Ç–æ–¥–∞ {method.__name__}: {image_url}")
                    
                    if full_text and len(full_text) > 150:
                        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π –º–µ—Ç–æ–¥: {method.__name__}")
                        if not best_result or len(full_text) > len(best_result):
                            best_result = full_text
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ú–µ—Ç–æ–¥ {method.__name__} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
                    continue
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ —Å –ª—É—á—à–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
            if best_result:
                best_image = self.select_best_image(all_found_images) if all_found_images else ""
                logger.info(f"‚úÖ –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º: {best_image}")
                return best_result, best_image
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç, –Ω–æ –Ω–∞—à–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è - –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if not best_result and all_found_images:
                logger.info("üñºÔ∏è –ù–∞–π–¥–µ–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ –Ω–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç. –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
                image_only = await self.extract_image_only(url)
                if image_only and image_only not in all_found_images:
                    all_found_images.append(image_only)
            
            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö
            best_image = self.select_best_image(all_found_images) if all_found_images else ""
            
            # Fallback - –∏—Å–ø–æ–ª—å–∑—É–µ–º RSS –æ–ø–∏—Å–∞–Ω–∏–µ
            rss_text, rss_image = await self.alternative_content_fetch(url)
            if rss_text and len(rss_text) > 100:
                logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ RSS –æ–ø–∏—Å–∞–Ω–∏–µ")
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ RSS –µ—Å–ª–∏ –µ—Å—Ç—å
                if rss_image and rss_image not in all_found_images:
                    all_found_images.append(rss_image)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                best_image = self.select_best_image(all_found_images) if all_found_images else ""
                logger.info(f"üñºÔ∏è –§–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {best_image}")
                return rss_text, best_image
            
            logger.info(f"üñºÔ∏è –§–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {best_image}")
            return "", best_image
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            # –ü—Ä–∏ –æ—à–∏–±–∫–µ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ö–æ—Ç—è –±—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_url = await self.extract_image_only(url)
            rss_text, rss_image = await self.alternative_content_fetch(url)
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            all_images = []
            if image_url:
                all_images.append(image_url)
            if rss_image:
                all_images.append(rss_image)
            
            best_image = self.select_best_image(all_images) if all_images else ""
            return rss_text, best_image

    async def extract_image_only(self, url):
        """–û—Ç–¥–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            logger.info(f"üîç –û—Ç–¥–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è: {url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            }
            
            response = self.session.get(url, headers=headers, timeout=20, verify=False)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –í–°–ï–ú–ò —Å–ø–æ—Å–æ–±–∞–º–∏
            image_url = self.extract_all_possible_images(soup, url)
            
            logger.info(f"üñºÔ∏è –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {image_url}")
            return image_url
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return ""

    async def method_smart_request(self, url):
        """–£–º–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –æ–±—Ö–æ–¥–æ–º –∑–∞—â–∏—Ç—ã"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        
        await asyncio.sleep(random.uniform(2, 4))
        
        try:
            response = self.session.get(url, headers=headers, timeout=30, verify=False)
            response.raise_for_status()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            if any(word in response.text.lower() for word in ['captcha', 'cloudflare', 'access denied', 'bot']):
                raise Exception("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞—â–∏—Ç–∞")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –í–°–ï–ú–ò —Å–ø–æ—Å–æ–±–∞–º–∏
            image_url = self.extract_all_possible_images(soup, url)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            content = self.extract_content(soup)
            
            return content, image_url
            
        except Exception as e:
            raise Exception(f"Smart request failed: {e}")

    async def method_simple_request(self, url):
        """–ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å"""
        headers = {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
        
        await asyncio.sleep(random.uniform(1, 2))
        
        try:
            response = self.session.get(url, headers=headers, timeout=20, verify=False)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –í–°–ï–ú–ò —Å–ø–æ—Å–æ–±–∞–º–∏
            image_url = self.extract_all_possible_images(soup, url)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            content = self.extract_content(soup)
            
            return content, image_url
            
        except Exception as e:
            raise Exception(f"Simple request failed: {e}")

    def extract_all_possible_images(self, soup, base_url):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –í–°–ï–ú–ò –≤–æ–∑–º–æ–∂–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏"""
        logger.info("üîç –ù–∞—á–∏–Ω–∞—é –ü–û–õ–ù–´–ô –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ...")
        
        found_images = []
        
        # 1. –ú–µ—Ç–∞-—Ç–µ–≥–∏ (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π)
        meta_selectors = [
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
            'meta[itemprop="image"]',
            'meta[name="og:image:url"]',
            'meta[property="twitter:image:src"]',
            'link[rel="image_src"]',
            'link[rel="apple-touch-icon"]',
            'link[rel="apple-touch-startup-image"]',
        ]
        
        for selector in meta_selectors:
            elements = soup.select(selector)
            for element in elements:
                image_url = element.get('content') or element.get('href') or element.get('src')
                if image_url:
                    normalized_url = self.normalize_image_url(image_url, base_url)
                    if normalized_url and normalized_url not in found_images:
                        found_images.append(normalized_url)
                        logger.info(f"‚úÖ –ú–µ—Ç–∞-—Ç–µ–≥ {selector}: {normalized_url}")
        
        # 2. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç–∞—Ç—å–∏ iXBT
        article_selectors = [
            'div.b-article img',
            'article img',
            '.b-article__text img',
            '.article-content img',
            '.post-content img',
            '.entry-content img',
            '.article-body img',
            'figure img',
            '.b-article__image img',
            '.article-image',
            '.wp-block-image img',
            '.content img',
            'main img',
            '.news-img',
            '.post-thumbnail img',
            '.entry-thumbnail img',
        ]
        
        for selector in article_selectors:
            elements = soup.select(selector)
            for element in elements:
                # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                for attr in ['src', 'data-src', 'data-lazy-src', 'data-original', 'data-srcset', 'srcset']:
                    image_url = element.get(attr)
                    if image_url:
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º srcset
                        if attr == 'srcset' and ',' in image_url:
                            image_url = image_url.split(',')[0].split(' ')[0]
                        
                        normalized_url = self.normalize_image_url(image_url, base_url)
                        if normalized_url and normalized_url not in found_images:
                            found_images.append(normalized_url)
                            logger.info(f"‚úÖ –°—Ç–∞—Ç—å—è {selector} [{attr}]: {normalized_url}")
        
        # 3. –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        all_images = soup.find_all('img')
        for img in all_images:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∫–æ–Ω–∫–∏, –ª–æ–≥–æ—Ç–∏–ø—ã –∏ –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            src = img.get('src', '')
            if any(ignore in src.lower() for ignore in ['logo', 'icon', 'avatar', 'spacer', 'pixel', 'emoji', 'favicon']):
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            width = img.get('width')
            height = img.get('height')
            if width and height:
                try:
                    if int(width) < 100 or int(height) < 100:
                        continue
                except:
                    pass
            
            # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –∞—Ç—Ä–∏–±—É—Ç—ã
            for attr in ['src', 'data-src', 'data-lazy-src', 'data-original']:
                image_url = img.get(attr)
                if image_url:
                    normalized_url = self.normalize_image_url(image_url, base_url)
                    if normalized_url and normalized_url not in found_images and self.is_valid_image_url(normalized_url):
                        found_images.append(normalized_url)
                        logger.info(f"‚úÖ –û–±—â–∏–π –ø–æ–∏—Å–∫ [{attr}]: {normalized_url}")
        
        # 4. –ò—â–µ–º –≤ —Å—Ç–∏–ª—è—Ö (background-image)
        styles = soup.find_all(style=re.compile(r'background-image'))
        for style in styles:
            style_content = style.get('style', '')
            urls = re.findall(r'url\([\'"]?(.*?)[\'"]?\)', style_content)
            for image_url in urls:
                normalized_url = self.normalize_image_url(image_url, base_url)
                if normalized_url and normalized_url not in found_images:
                    found_images.append(normalized_url)
                    logger.info(f"‚úÖ CSS background: {normalized_url}")
        
        # 5. –ò—â–µ–º –≤ JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        script_tags = soup.find_all('script', type='application/ld+json')
        for script in script_tags:
            try:
                data = json.loads(script.string)
                images = self.extract_images_from_jsonld(data)
                for image_url in images:
                    normalized_url = self.normalize_image_url(image_url, base_url)
                    if normalized_url and normalized_url not in found_images:
                        found_images.append(normalized_url)
                        logger.info(f"‚úÖ JSON-LD: {normalized_url}")
            except:
                pass
        
        logger.info(f"üéØ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(found_images)}")
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ —Ä–∞–∑–º–µ—Ä—É –∏ –∫–∞—á–µ—Å—Ç–≤—É)
        if found_images:
            best_image = self.select_best_image(found_images)
            logger.info(f"üèÜ –í—ã–±—Ä–∞–Ω–æ –ª—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {best_image}")
            return best_image
        
        logger.warning("‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
        return ""

    def extract_images_from_jsonld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ JSON-LD –¥–∞–Ω–Ω—ã—Ö"""
        images = []
        
        if isinstance(data, dict):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
            for key in ['image', 'thumbnail', 'photo', 'logo']:
                if key in data:
                    image_data = data[key]
                    if isinstance(image_data, str):
                        images.append(image_data)
                    elif isinstance(image_data, dict) and 'url' in image_data:
                        images.append(image_data['url'])
                    elif isinstance(image_data, list):
                        for item in image_data:
                            if isinstance(item, str):
                                images.append(item)
                            elif isinstance(item, dict) and 'url' in item:
                                images.append(item['url'])
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            for value in data.values():
                if isinstance(value, (dict, list)):
                    images.extend(self.extract_images_from_jsonld(value))
        
        elif isinstance(data, list):
            for item in data:
                images.extend(self.extract_images_from_jsonld(item))
        
        return images

    def select_best_image(self, image_urls):
        """–í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö"""
        if not image_urls:
            return ""
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º
        extension_priority = ['.jpg', '.jpeg', '.png', '.webp', '.gif']
        
        for ext in extension_priority:
            for url in image_urls:
                if url.lower().endswith(ext):
                    return url
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ URL
        priority_keywords = ['large', 'big', 'main', 'featured', 'cover', 'hero']
        for keyword in priority_keywords:
            for url in image_urls:
                if keyword in url.lower():
                    return url
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        return image_urls[0]

    def extract_content(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in soup(["script", "style", "nav", "header", "footer", "aside", "form"]):
            element.decompose()
        
        # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –ø–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º iXBT
        selectors = [
            'div.b-article__text',
            'article.b-article',
            'div.b-article-body',
            'div.article-content',
            'div.post-content',
            'div.entry-content',
            'div.content',
            'article',
            'div.article__text',
            'div.article-body',
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                text = self.clean_and_extract_text(element)
                if len(text) > 200:
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É: {selector}")
                    return text
        
        # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Ç–µ–≥–∞–º
        semantic_tags = ['main', 'article', 'div[role="main"]']
        for tag in semantic_tags:
            if tag.startswith('div'):
                element = soup.find('div', role='main')
            else:
                element = soup.find(tag)
                
            if element:
                text = self.clean_and_extract_text(element)
                if len(text) > 200:
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ: {tag}")
                    return text
        
        return ""

    def clean_and_extract_text(self, element):
        """–û—á–∏—Å—Ç–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        # –ö–ª–æ–Ω–∏—Ä—É–µ–º —ç–ª–µ–º–µ–Ω—Ç
        element = BeautifulSoup(str(element), 'html.parser')
        
        # –£–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        garbage_selectors = [
            'div.ad', 'div.adv', 'div.advertisement', 'div.banner',
            'div.comments', 'div.social', 'div.share', 'div.related',
            'div.recommended', 'div.teaser', 'div.meta', 'div.tags',
            'ins', 'iframe', 'a[href*="ad"]', 'div[class*="ad"]',
            'div[class*="banner"]', 'div.widget', 'div.subscribe',
            'div.navigation', 'div.pagination', 'div.author',
        ]
        
        for selector in garbage_selectors:
            for elem in element.select(selector):
                elem.decompose()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        paragraphs = element.find_all('p')
        text_parts = []
        
        for p in paragraphs:
            text = p.get_text(strip=True)
            if self.is_meaningful_text(text):
                text_parts.append(text)
        
        text = '\n'.join(text_parts)
        return self.post_process_text(text)

    def is_meaningful_text(self, text):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º"""
        if len(text) < 40:
            return False
            
        garbage_indicators = [
            '—Ä–µ–∫–ª–∞–º–∞', '–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å', '–∏—Å—Ç–æ—á–Ω–∏–∫:', '—á–∏—Ç–∞—Ç—å —Ç–∞–∫–∂–µ',
            '–∫–æ–º–º–µ–Ω—Ç–∞—Ä', '—Ñ–æ—Ç–æ:', '–≤–∏–¥–µ–æ:', '—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ', 'share',
            '—Ç–µ–≥–∏:', '–æ—Ü–µ–Ω–∏—Ç–µ —Å—Ç–∞—Ç—å—é', '–ø–æ–¥–µ–ª–∏—Ç—å—Å—è', '—Ä–µ–¥–∞–∫—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç',
        ]
        
        if any(indicator in text.lower() for indicator in garbage_indicators):
            return False
            
        return len(text.split()) >= 5

    def post_process_text(self, text):
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º URL
        text = re.sub(r'https?://\S+', '', text)
        
        # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
        text = re.sub(r'<[^>]+>', '', text)
        
        return text.strip()

    def is_valid_image_url(self, url):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ URL –ø–æ—Ö–æ–∂ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
        if not url or url.strip() == '':
            return False
        
        url_lower = url.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        valid_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp', '.tiff', '.svg']
        if any(url_lower.endswith(ext) for ext in valid_extensions):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ URL
        image_patterns = [
            '/images/', '/img/', '/uploads/', '/media/',
            'image', 'photo', 'picture', 'img', 'upload'
        ]
        if any(pattern in url_lower for pattern in image_patterns):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        if any(param in url_lower for param in ['/wp-content/', '/content/images/']):
            return True
        
        return False

    def normalize_image_url(self, image_url, base_url):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not image_url:
            return ""
        
        # –û—á–∏—â–∞–µ–º URL –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –∫–æ–¥–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        image_url = image_url.strip()
        image_url = image_url.replace(' ', '%20')  # –ö–æ–¥–∏—Ä—É–µ–º –ø—Ä–æ–±–µ–ª—ã
        
        # –£–¥–∞–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∏ —è–∫–æ—Ä—è
        image_url = image_url.split('?')[0].split('#')[0]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL
        if image_url.startswith('//'):
            image_url = 'https:' + image_url
        elif image_url.startswith('/'):
            image_url = 'https://www.ixbt.com' + image_url
        elif not image_url.startswith(('http://', 'https://')):
            # –ï—Å–ª–∏ URL –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –±–µ–∑ —Å–ª–µ—à–∞
            if image_url.startswith('./'):
                image_url = image_url[2:]
            base_domain = 'https://www.ixbt.com'
            image_url = base_domain + '/' + image_url.lstrip('/')
        
        return image_url

    async def alternative_content_fetch(self, url):
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ RSS"""
        try:
            headers = {
                'User-Agent': self.ua.random,
                'Accept': 'application/rss+xml,application/xml;q=0.9,*/*;q=0.8',
            }
            
            response = self.session.get(IXBT_RSS_URL, headers=headers, timeout=20)
            feed = feedparser.parse(response.content)
            
            for entry in feed.entries:
                if entry.link == url and hasattr(entry, 'summary'):
                    soup = BeautifulSoup(entry.summary, 'html.parser')
                    text = soup.get_text(strip=True)
                    
                    # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ RSS –æ–ø–∏—Å–∞–Ω–∏–∏
                    image_url = ""
                    img_tag = soup.find('img')
                    if img_tag and img_tag.get('src'):
                        image_url = self.normalize_image_url(img_tag.get('src'), url)
                    
                    if len(text) > 100:
                        logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ RSS –æ–ø–∏—Å–∞–Ω–∏–µ")
                        return text, image_url
            
            return "", ""
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è: {e}")
            return "", ""

    async def download_image(self, image_url, filename):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            if not image_url:
                logger.warning("‚ùå URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—É—Å—Ç–æ–π")
                return False
                
            logger.info(f"üñºÔ∏è –°–∫–∞—á–∏–≤–∞—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
                'Referer': 'https://www.ixbt.com/',
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∞–π–º–∞—É—Ç—ã –∏ stream –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
            response = self.session.get(
                image_url, 
                headers=headers, 
                timeout=30, 
                verify=False,
                stream=True
            )
            response.raise_for_status()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º content-type
            content_type = response.headers.get('content-type', '').lower()
            logger.info(f"üìã Content-Type: {content_type}")
            
            if 'image' not in content_type:
                logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π content-type: {content_type}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            with open(filename, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            file_size = os.path.getsize(filename)
            logger.info(f"üì¶ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size} –±–∞–π—Ç")
            
            if file_size < 1024:  # –ú–∏–Ω–∏–º—É–º 1KB
                logger.warning(f"‚ö†Ô∏è –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π: {file_size} –±–∞–π—Ç")
                os.remove(filename)
                return False
            
            # –ü—Ä–æ–±—É–µ–º –æ—Ç–∫—Ä—ã—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
            try:
                with Image.open(filename) as img:
                    img.verify()  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–∞
                logger.info(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–∫–∞—á–∞–Ω–æ –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ: {filename}")
                return True
            except Exception as img_error:
                logger.error(f"‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {img_error}")
                os.remove(filename)
                return False
                
        except requests.exceptions.Timeout:
            logger.error("‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return False
        except requests.exceptions.ConnectionError:
            logger.error("‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return False
        except requests.exceptions.HTTPError as e:
            logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ {e.response.status_code}: {e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
            return False

    def create_news_image(self, title, filename):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Ç–æ–ª—å–∫–æ –∫–∞–∫ fallback)"""
        try:
            width, height = 1200, 630
            
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.new('RGB', (width, height), color=(30, 30, 46))
            draw = ImageDraw.Draw(image)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç
            for y in range(height):
                r = int(30 + (50 * y / height))
                g = int(30 + (40 * y / height))
                b = int(46 + (50 * y / height))
                draw.line([(0, y), (width, y)], fill=(r, g, b))
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
            try:
                # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π —à—Ä–∏—Ñ—Ç
                font = ImageFont.load_default()
                title_lines = self.wrap_text(title, font, width - 100)
                
                # –†–∏—Å—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                y_position = height // 3
                for line in title_lines:
                    bbox = draw.textbbox((0, 0), line, font=font)
                    text_width = bbox[2] - bbox[0]
                    x_position = (width - text_width) // 2
                    draw.text((x_position, y_position), line, fill=(255, 255, 255), font=font)
                    y_position += bbox[3] - bbox[1] + 10
            except:
                pass
            
            image.save(filename, 'JPEG', quality=90)
            logger.info(f"üñºÔ∏è –°–æ–∑–¥–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (fallback): {filename}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–µ–π—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            try:
                Image.new('RGB', (800, 400), color=(90, 100, 110)).save(filename)
                return True
            except:
                return False

    def wrap_text(self, text, font, max_width):
        """–ü–µ—Ä–µ–Ω–æ—Å —Ç–µ–∫—Å—Ç–∞"""
        words = text.split()
        lines = []
        current_line = []
        
        for word in words:
            test_line = ' '.join(current_line + [word])
            bbox = ImageDraw.Draw(Image.new('RGB', (1, 1))).textbbox((0, 0), test_line, font=font)
            text_width = bbox[2] - bbox[0]
            
            if text_width <= max_width:
                current_line.append(word)
            else:
                lines.append(' '.join(current_line))
                current_line = [word]
        
        if current_line:
            lines.append(' '.join(current_line))
        
        return lines[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 —Å—Ç—Ä–æ–∫–∏

    def rephrase_text(self, text, title):
        """–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = re.split(r'[.!?]+', text)
            meaningful_sentences = []
            
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) < 30:
                    continue
                    
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º—É—Å–æ—Ä
                if any(word in sentence.lower() for word in [
                    '—Ä–µ–∫–ª–∞–º–∞', '–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å', '–∏—Å—Ç–æ—á–Ω–∏–∫:', '–∫–æ–º–º–µ–Ω—Ç–∞—Ä'
                ]):
                    continue
                    
                meaningful_sentences.append(sentence)
            
            if not meaningful_sentences:
                return ""
            
            # –ë–µ—Ä–µ–º 3 —Å–∞–º—ã—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            selected_sentences = meaningful_sentences[:3]
            
            # –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–µ–º
            rephrased_sentences = []
            for sentence in selected_sentences:
                rephrased = self.rephrase_sentence(sentence)
                if rephrased:
                    rephrased_sentences.append(rephrased)
            
            if not rephrased_sentences:
                return ""
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º
            summary = self.format_sentences_properly(rephrased_sentences)
            
            return summary
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return text[:400] + '...'

    def rephrase_sentence(self, sentence):
        """–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        try:
            # –£–±–∏—Ä–∞–µ–º —É–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫
            sentence = re.sub(r'\([^)]*–∏—Å—Ç–æ—á–Ω–∏–∫[^)]*\)', '', sentence, flags=re.IGNORECASE)
            sentence = re.sub(r'–ø–æ –¥–∞–Ω–Ω—ã–º[^,.]+', '', sentence, flags=re.IGNORECASE)
            sentence = re.sub(r'—Å–æ–æ–±—â–∞–µ—Ç[^,.]+', '', sentence, flags=re.IGNORECASE)
            
            # –ó–∞–º–µ–Ω—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
            synonyms = {
                '—Å–æ–æ–±—â–∞–µ—Ç—Å—è': '–ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏',
                '–∑–∞—è–≤–∏–ª': '–æ—Ç–º–µ—Ç–∏–ª',
                '–æ–±—ä—è–≤–∏–ª': '—Å–æ–æ–±—â–∏–ª',
                '—Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª': '—Å–æ–∑–¥–∞–ª',
                '–ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª': '–ø–æ–∫–∞–∑–∞–ª',
                '–∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª': '–æ–±—ä—è–≤–∏–ª –æ',
                '–∫–æ–º–ø–∞–Ω–∏—è': '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è',
                '–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è': '–∫–æ–º–ø–∞–Ω–∏—è',
                '—Å—Ç–∞—Ä—Ç–∞–ø': '–Ω–æ–≤–∞—è –∫–æ–º–ø–∞–Ω–∏—è',
            }
            
            words = sentence.split()
            rephrased_words = []
            
            for word in words:
                lower_word = word.lower()
                if lower_word in synonyms:
                    rephrased_words.append(synonyms[lower_word])
                else:
                    rephrased_words.append(word)
            
            rephrased = ' '.join(rephrased_words)
            
            return rephrased.strip()
            
        except Exception as e:
            return sentence

    def format_sentences_properly(self, sentences):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        if not sentences:
            return ""
        
        formatted_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            if not sentence.endswith(('.', '!', '?')):
                sentence += '.'
            
            if sentence and len(sentence) > 1:
                sentence = sentence[0].upper() + sentence[1:]
            
            formatted_sentences.append(sentence)
        
        result = ' '.join(formatted_sentences)
        result = re.sub(r'\s+', ' ', result)
        
        return result.strip()

    def create_news_post(self, news_item):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å—Ç–∞ –¥–ª—è Telegram —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        title = news_item['title']
        original_text = news_item['full_text']
        
        logger.info(f"üéØ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –Ω–æ–≤–æ—Å—Ç—å: {title}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        banned_orgs = self.check_banned_organizations(title, original_text)
        
        warning_text = ""
        if banned_orgs:
            warning_text = "\n\n‚ö†Ô∏è *–í–ù–ò–ú–ê–ù–ò–ï:* –í –Ω–æ–≤–æ—Å—Ç–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, "
            warning_text += "–∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –†–§"
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (–ø–µ—Ä–≤—ã–µ 3)
            orgs_list = banned_orgs[:3]
            warning_text += f"\n–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏—è: {', '.join(orgs_list)}"
            
            if len(banned_orgs) > 3:
                warning_text += f" –∏ –µ—â—ë {len(banned_orgs) - 3}"
            
            logger.warning(f"üö´ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏: {banned_orgs}")
        
        # –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        rephrased_text = self.rephrase_text(original_text, title)
        
        if not rephrased_text or len(rephrased_text.strip()) < 80:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç")
            return None
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ —Ö–µ—à—Ç–µ–≥–∏
        categories = self.detect_news_category(title, rephrased_text)
        hashtags = self.get_hashtags_for_category(categories)
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å—Ç —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
        post_text = self.format_post(title, rephrased_text, hashtags, warning_text)
        
        return post_text

   def format_post(self, title, text, hashtags, warning_text=""):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å—Ç–∞ —Å –≤–æ–∑–º–æ–∂–Ω—ã–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º"""
        clean_title = re.sub(r'<[^>]+>', '', title.strip())
        clean_text = re.sub(r'<[^>]+>', '', text)
        clean_text = re.sub(r'\s+', ' ', clean_text.strip())
        
        # –•–µ—à—Ç–µ–≥–∏ —É–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç #, –ø—Ä–æ—Å—Ç–æ –æ–±—ä–µ–¥–∏–Ω—è–µ–º
        hashtags_str = ' '.join(hashtags)
        
        # –ñ–∏—Ä–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å –ø–æ–º–æ—â—å—é Markdown
        post = f"*{clean_title}*\n\n{clean_text}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if warning_text:
            post += warning_text
        
        post += f"\n\n{hashtags_str}"
        
        return post

    def escape_markdown_v2(self, text):
        """–≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è MarkdownV2"""
        escape_chars = r'_*[]()~`>#+-=|{}.!'
        
        def escape_text(text_to_escape):
            return re.sub(f'([{re.escape(escape_chars)}])', r'\\\1', text_to_escape)
        
        lines = text.split('\n')
        escaped_lines = []
        
        for line in lines:
            escaped_lines.append(escape_text(line))
        
        return '\n'.join(escaped_lines)

   def format_telegram_post(self, post_text):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Telegram —Å —É—á–µ—Ç–æ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π"""
        try:
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–∞—Å—Ç–∏
            parts = post_text.split('\n\n')
            
            if len(parts) >= 3:
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å) - –æ—Å—Ç–∞–≤–ª—è–µ–º –∂–∏—Ä–Ω—ã–π
                title_line = parts[0]
                
                # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç (–≤—Ç–æ—Ä–∞—è —á–∞—Å—Ç—å) - —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º
                text_content = parts[1]
                escaped_text = self.escape_markdown_v2(text_content)
                
                # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Ç—Ä–µ—Ç—å–µ–π —á–∞—Å—Ç–∏)
                warning_content = ""
                hashtags = parts[-1]  # –•–µ—à—Ç–µ–≥–∏ –≤—Å–µ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ, –æ–Ω–æ –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥ —Ö–µ—à—Ç–µ–≥–∞–º–∏
                if len(parts) >= 4 and "–í–ù–ò–ú–ê–ù–ò–ï" in parts[2]:
                    warning_content = parts[2]
                    # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ö–µ—à—Ç–µ–≥–∏ –∫–∞–∫ –µ—Å—Ç—å
                    escaped_warning = self.escape_markdown_v2(warning_content)
                    formatted = f"{title_line}\n\n{escaped_text}\n\n{escaped_warning}\n\n{hashtags}"
                else:
                    formatted = f"{title_line}\n\n{escaped_text}\n\n{hashtags}"
            else:
                # –ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è, —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
                lines = post_text.split('\n')
                if len(lines) >= 3:
                    title_line = lines[0]
                    text_content = '\n'.join(lines[1:-1])
                    hashtags = lines[-1]
                    escaped_text = self.escape_markdown_v2(text_content)
                    formatted = f"{title_line}\n\n{escaped_text}\n\n{hashtags}"
                else:
                    # –ü—Ä–æ—Å—Ç–æ —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫—Ä–æ–º–µ —Ö–µ—à—Ç–µ–≥–æ–≤
                    formatted = self.escape_markdown_v2(post_text)
            
            if len(formatted) > 1024:
                # –£–ø—Ä–æ—â–∞–µ–º —Ç–µ–∫—Å—Ç –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
                lines = formatted.split('\n')
                if len(lines) > 2:
                    short_text = lines[2][:300] + '...'
                    simplified = f"{lines[0]}\n\n{short_text}"
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –∏ —Ö–µ—à—Ç–µ–≥–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
                    if len(lines) > 4 and "–í–ù–ò–ú–ê–ù–ò–ï" in lines[4]:
                        simplified += f"\n\n{lines[4]}\n\n{lines[-1]}"
                    elif len(lines) > 3:
                        simplified += f"\n\n{lines[-1]}"
                    
                    return simplified
            
            return formatted
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            # –ü—Ä–æ—Å—Ç–æ–π fallback –±–µ–∑ Markdown
            simple_text = re.sub(r'[*_`\[\]()~>#+\-=|{}.!]', '', post_text)
            return simple_text[:900] + "\n\n#–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–ù–æ–≤–æ—Å—Ç–∏"

    async def publish_to_telegram(self, post_text, image_path, news_hash):
        """–ü—É–±–ª–∏–∫–∞—Ü–∏—è –≤ Telegram"""
        for attempt in range(3):
            try:
                formatted_text = self.format_telegram_post(post_text)
                
                logger.info(f"üì§ –ü—É–±–ª–∏–∫–∞—Ü–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                
                with open(image_path, 'rb') as photo:
                    await self.bot.send_photo(
                        chat_id=CHANNEL_ID,
                        photo=photo,
                        caption=formatted_text,
                        parse_mode='MarkdownV2'
                    )
                
                logger.info("‚úÖ –ü–æ—Å—Ç –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω")
                
                # –£–¥–∞–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                try:
                    if os.path.exists(image_path):
                        os.remove(image_path)
                        logger.info(f"üóëÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–¥–∞–ª–µ–Ω–æ: {image_path}")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
                
                return True
                
            except TelegramError as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ Telegram: {e}")
                
                if attempt == 2:
                    try:
                        plain_text = re.sub(r'[*_`\[\]()~>#+\-=|{}.!]', '', post_text)
                        plain_text = plain_text[:800] + "\n\n#–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ #–ù–æ–≤–æ—Å—Ç–∏"
                        
                        with open(image_path, 'rb') as photo:
                            await self.bot.send_photo(
                                chat_id=CHANNEL_ID,
                                photo=photo,
                                caption=plain_text
                            )
                        logger.info("‚úÖ –ü–æ—Å—Ç –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
                        
                        # –£–¥–∞–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                        try:
                            if os.path.exists(image_path):
                                os.remove(image_path)
                                logger.info(f"üóëÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–¥–∞–ª–µ–Ω–æ: {image_path}")
                        except Exception as e:
                            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
                        
                        return True
                    except Exception as e2:
                        logger.error(f"‚ùå –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞: {e2}")
                
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")
                await asyncio.sleep(2)
        
        return False

    async def process_news_cycle(self):
        """–¶–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            logger.info("üîÑ –ü–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")
            
            news_list = await self.fetch_news()
            
            if not news_list:
                logger.info("‚ÑπÔ∏è –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç")
                return
                
            for news_item in news_list:
                logger.info(f"üìù –û–±—Ä–∞–±–æ—Ç–∫–∞: {news_item['title']}")
                logger.info(f"üîó –°—Å—ã–ª–∫–∞: {news_item['link']}")
                logger.info(f"üñºÔ∏è URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {news_item['image_url']}")
                
                post_text = self.create_news_post(news_item)
                
                if not post_text:
                    continue
                
                # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏
                Path("images").mkdir(exist_ok=True)
                Path("downloaded_images").mkdir(exist_ok=True)
                
                image_filename = f"downloaded_images/news_{news_item['hash']}.jpg"
                fallback_image_filename = f"images/news_{news_item['hash']}.jpg"
                
                # –ü—Ä–æ–±—É–µ–º —Å–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Å–∞–π—Ç–∞
                image_downloaded = False
                if news_item['image_url']:
                    logger.info(f"üñºÔ∏è –ü—ã—Ç–∞—é—Å—å —Å–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {news_item['image_url']}")
                    image_downloaded = await self.download_image(news_item['image_url'], image_filename)
                else:
                    logger.warning("‚ùå URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –Ω–æ–≤–æ—Å—Ç–∏")
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–∞—á–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º fallback
                if image_downloaded:
                    logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏")
                    final_image_path = image_filename
                else:
                    logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, —Å–æ–∑–¥–∞—é fallback")
                    if self.create_news_image(news_item['title'], fallback_image_filename):
                        final_image_path = fallback_image_filename
                    else:
                        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å")
                        continue
                
                # –ü—É–±–ª–∏–∫—É–µ–º
                success = await self.publish_to_telegram(
                    post_text, final_image_path, news_item['hash']
                )
                
                if success:
                    self.processed_news.add(news_item['hash'])
                    self.save_processed_news()
                    logger.info(f"‚úÖ –ù–æ–≤–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {news_item['hash']}")
                    await asyncio.sleep(10)
                else:
                    logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ü–∏–∫–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±–ª–∞—á–Ω–æ–≥–æ —Ö–æ—Å—Ç–∏–Ω–≥–∞"""
    bot = SmartNewsBot()
    
    logger.info("üöÇ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –Ω–∞ Railway!")
    logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(bot.processed_news)}")
    logger.info(f"üîß –¢–æ–∫–µ–Ω: {'—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω' if TOKEN else '–Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω'}")
    logger.info(f"üì¢ –ö–∞–Ω–∞–ª: {CHANNEL_ID}")
    
    # –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª –¥–ª—è –æ–±–ª–∞—á–Ω–æ–≥–æ —Ö–æ—Å—Ç–∏–Ω–≥–∞
    while True:
        try:
            await bot.process_news_cycle()
            logger.info(f"üí§ –û–∂–∏–¥–∞–Ω–∏–µ {CHECK_INTERVAL} —Å–µ–∫...")
            await asyncio.sleep(CHECK_INTERVAL)
            
        except KeyboardInterrupt:
            logger.info("üõë –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
            import traceback
            logger.error(f"üîç –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")
            logger.info("üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ 60 —Å–µ–∫—É–Ω–¥...")
            await asyncio.sleep(60)

if __name__ == '__main__':
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ä–µ–¥–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    print(f"üêç Python version: {sys.version}")
    print(f"üöÇ Railway environment: {'RAILWAY_ENVIRONMENT' in os.environ}")
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"–§–∞—Ç–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)