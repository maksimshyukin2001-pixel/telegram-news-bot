import asyncio
import logging
import hashlib
import json
import os
import sys
import re
import random
import requests
import time
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import feedparser
from PIL import Image, ImageDraw, ImageFont
from telegram import Bot
from telegram.error import TelegramError
from fake_useragent import UserAgent
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import io
import banned_organizations
import news_tags

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è - —Ç–µ–ø–µ—Ä—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –±–ª–æ–∫
if 'RAILWAY_ENVIRONMENT' in os.environ or 'RAILWAY_STATIC_URL' in os.environ:
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è Railway
    TOKEN = os.environ.get('TELEGRAM_TOKEN', "7445394461:AAGHiGYBiCwEg-tbchU9lOJmywv4CjcKuls")
    CHANNEL_ID = os.environ.get('TELEGRAM_CHANNEL', "@techno_met")
else:
    # –õ–æ–∫–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    TOKEN = "7445394461:AAGHiGYBiCwEg-tbchU9lOJmywv4CjcKuls"
    CHANNEL_ID = "@techno_met"

IXBT_RSS_URL = "https://www.ixbt.com/export/news.rss"
CHECK_INTERVAL = 1800  # 30 –º–∏–Ω—É—Ç

# –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –æ–±–ª–∞–∫–µ
def ensure_directories():
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
    directories = ['images', 'downloaded_images']
    for directory in directories:
        os.makedirs(directory, exist_ok=True)

# –í—ã–∑–æ–≤ –≤ –Ω–∞—á–∞–ª–µ
ensure_directories()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ httpx
logging.getLogger('httpx').setLevel(logging.WARNING)

class SmartNewsBot:
    def __init__(self):
        self.processed_news = set()
        self.load_processed_news()
        self.bot = Bot(token=TOKEN)
        self.ua = UserAgent()
        self.session = self.create_advanced_session()
        
    def create_advanced_session(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Å–µ—Å—Å–∏–∏ —Å –æ–±—Ö–æ–¥–æ–º –∑–∞—â–∏—Ç—ã"""
        session = requests.Session()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º retry —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"],
            backoff_factor=1
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        return session

    def load_processed_news(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists('processed_news.json'):
                with open('processed_news.json', 'r') as f:
                    data = json.load(f)
                    self.processed_news = set(data)
        except Exception as e:
            logger.error(f"Error loading processed news: {e}")
            self.processed_news = set()

    def save_processed_news(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Ñ–∞–π–ª"""
        try:
            with open('processed_news.json', 'w') as f:
                json.dump(list(self.processed_news), f)
        except Exception as e:
            logger.error(f"Error saving processed news: {e}")

    def get_news_hash(self, title, link):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ö—ç—à–∞ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏"""
        return hashlib.md5(f"{title}{link}".encode()).hexdigest()

    def check_banned_organizations(self, title, text):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        content = f"{title} {text}".lower()
        
        found_organizations = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        for org in banned_organizations.BANNED_ORGANIZATIONS:
            if org.lower() in content:
                found_organizations.append(org)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in banned_organizations.BANNED_KEYWORDS:
            if keyword in content:
                # –ò—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
                start = max(0, content.find(keyword) - 50)
                end = min(len(content), content.find(keyword) + len(keyword) + 50)
                context = content[start:end]
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–æ–∑–º–æ–∂–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                words = context.split()
                if len(words) > 2:
                    potential_org = ' '.join(words[:min(5, len(words))])
                    found_organizations.append(f"–∫–æ–Ω—Ç–µ–∫—Å—Ç: {potential_org}...")
        
        return found_organizations

    def detect_news_category(self, title, text):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ —Ö–µ—à—Ç–µ–≥–æ–≤"""
        title_lower = title.lower()
        text_lower = text.lower()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        full_text = f"{title_lower} {text_lower}"
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ news_tags.py
        categories = news_tags.CATEGORIES
        
        # –°—á–∏—Ç–∞–µ–º –≤–µ—Å –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
        for category, data in categories.items():
            weight = 0
            
            for keyword in data['keywords']:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –≥—Ä–∞–Ω–∏—Ü —Å–ª–æ–≤
                pattern = r'\b' + re.escape(keyword) + r'\b'
                
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏–º–µ–µ—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å (5 –±–∞–ª–ª–æ–≤)
                if re.search(pattern, title_lower):
                    weight += 5
                # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–º–µ–µ—Ç –º–µ–Ω—å—à–∏–π –≤–µ—Å (2 –±–∞–ª–ª–∞)
                elif re.search(pattern, full_text):
                    weight += 2
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è (—Å–Ω–∏–∂–∞–µ–º –≤–µ—Å –µ—Å–ª–∏ –µ—Å—Ç—å –∏—Å–∫–ª—é—á–∞—é—â–∏–µ —Å–ª–æ–≤–∞)
            if 'exclude' in data:
                for exclude_word in data['exclude']:
                    if exclude_word in full_text:
                        weight = max(0, weight - 3)  # –°—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞–µ–º –≤–µ—Å
            
            data['weight'] = weight
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –≤–µ—Å—É –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        def category_sort_key(item):
            category, data = item
            return (data['weight'], data.get('priority', 0))
        
        sorted_categories = sorted(categories.items(), key=category_sort_key, reverse=True)
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≤–µ—Å–æ–º > 3 –∏ –º–∞–∫—Å–∏–º—É–º 3 —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
        found_categories = [cat for cat, data in sorted_categories if data['weight'] > 3][:3]
        
        # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–±—É–µ–º —Å–Ω–∏–∑–∏—Ç—å –ø–æ—Ä–æ–≥
        if not found_categories:
            found_categories = [cat for cat, data in sorted_categories if data['weight'] > 1][:2]
        
        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–µ —Ç–µ–≥–∏
        if not found_categories:
            logger.info("‚ÑπÔ∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –æ–±—â–∏–µ —Ç–µ–≥–∏")
            return ['technology', 'news']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ —Ç–µ–≥–∏
        result_categories = found_categories + ['technology', 'news']
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        logger.info(f"üè∑Ô∏è –û–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {result_categories}")
        for cat in found_categories:
            logger.info(f"   - {cat}: –≤–µ—Å {categories[cat]['weight']}")
        
        return result_categories

    def get_hashtags_for_category(self, categories):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Ö–µ—à—Ç–µ–≥–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ news_tags.py
        return news_tags.get_all_hashtags(categories)

    async def fetch_news(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å IXBT"""
        try:
            headers = {
                'User-Agent': self.ua.random,
                'Accept': 'application/rss+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            }
            
            response = self.session.get(IXBT_RSS_URL, headers=headers, timeout=20)
            response.raise_for_status()
            
            feed = feedparser.parse(response.content)
            news_list = []
            
            for entry in feed.entries[:5]:
                news_hash = self.get_news_hash(entry.title, entry.link)
                
                if news_hash not in self.processed_news:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    await asyncio.sleep(random.uniform(2, 5))
                    
                    full_text, image_url = await self.get_full_article_text_and_image(entry.link)
                    
                    if len(full_text) > 150:
                        news_item = {
                            'title': entry.title,
                            'link': entry.link,
                            'summary': entry.summary,
                            'full_text': full_text,
                            'image_url': image_url,
                            'hash': news_hash,
                            'published': entry.published
                        }
                        news_list.append(news_item)
                        logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {entry.title}")
                        if image_url:
                            logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏: {image_url}")
                        else:
                            logger.warning("‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏")
                    else:
                        logger.warning(f"‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç: {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return news_list
        except Exception as e:
            logger.error(f"Error fetching news: {e}")
            return []

    async def get_full_article_text_and_image(self, url):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Å—Ç–∞—Ç—å–∏"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏ —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            methods = [
                self.method_smart_request,
                self.method_simple_request,
            ]
            
            best_result = None
            all_found_images = []  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–ï –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
            for method in methods:
                try:
                    full_text, image_url = await method(url)
                    logger.info(f"üîÑ –ú–µ—Ç–æ–¥ {method.__name__}: —Ç–µ–∫—Å—Ç={len(full_text) if full_text else 0}, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ={image_url}")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    if image_url and image_url not in all_found_images:
                        all_found_images.append(image_url)
                        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ –º–µ—Ç–æ–¥–∞ {method.__name__}: {image_url}")
                    
                    if full_text and len(full_text) > 150:
                        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π –º–µ—Ç–æ–¥: {method.__name__}")
                        if not best_result or len(full_text) > len(best_result):
                            best_result = full_text
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ú–µ—Ç–æ–¥ {method.__name__} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
                    continue
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ —Å –ª—É—á—à–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
            if best_result:
                best_image = self.select_best_image(all_found_images) if all_found_images else ""
                logger.info(f"‚úÖ –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º: {best_image}")
                return best_result, best_image
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç, –Ω–æ –Ω–∞—à–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è - –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if not best_result and all_found_images:
                logger.info("üñºÔ∏è –ù–∞–π–¥–µ–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ –Ω–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç. –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
                image_only = await self.extract_image_only(url)
                if image_only and image_only not in all_found_images:
                    all_found_images.append(image_only)
            
            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö
            best_image = self.select_best_image(all_found_images) if all_found_images else ""
            
            # Fallback - –∏—Å–ø–æ–ª—å–∑—É–µ–º RSS –æ–ø–∏—Å–∞–Ω–∏–µ
            rss_text, rss_image = await self.alternative_content_fetch(url)
            if rss_text and len(rss_text) > 100:
                logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ RSS –æ–ø–∏—Å–∞–Ω–∏–µ")
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ RSS –µ—Å–ª–∏ –µ—Å—Ç—å
                if rss_image and rss_image not in all_found_images:
                    all_found_images.append(rss_image)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                best_image = self.select_best_image(all_found_images) if all_found_images else ""
                logger.info(f"üñºÔ∏è –§–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {best_image}")
                return rss_text, best_image
            
            logger.info(f"üñºÔ∏è –§–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {best_image}")
            return "", best_image
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            # –ü—Ä–∏ –æ—à–∏–±–∫–µ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ö–æ—Ç—è –±—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_url = await self.extract_image_only(url)
            rss_text, rss_image = await self.alternative_content_fetch(url)
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            all_images = []
            if image_url:
                all_images.append(image_url)
            if rss_image:
                all_images.append(rss_image)
            
            best_image = self.select_best_image(all_images) if all_images else ""
            return rss_text, best_image

    async def extract_image_only(self, url):
        """–û—Ç–¥–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            logger.info(f"üîç –û—Ç–¥–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è: {url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            }
            
            response = self.session.get(url, headers=headers, timeout=20, verify=False)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –í–°–ï–ú–ò —Å–ø–æ—Å–æ–±–∞–º–∏
            image_url = self.extract_all_possible_images(soup, url)
            
            logger.info(f"üñºÔ∏è –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {image_url}")
            return image_url
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return ""

    async def method_smart_request(self, url):
        """–£–º–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –æ–±—Ö–æ–¥–æ–º –∑–∞—â–∏—Ç—ã"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        
        await asyncio.sleep(random.uniform(2, 4))
        
        try:
            response = self.session.get(url, headers=headers, timeout=30, verify=False)
            response.raise_for_status()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            if any(word in response.text.lower() for word in ['captcha', 'cloudflare', 'access denied', 'bot']):
                raise Exception("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞—â–∏—Ç–∞")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –í–°–ï–ú–ò —Å–ø–æ—Å–æ–±–∞–º–∏
            image_url = self.extract_all_possible_images(soup, url)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            content = self.extract_content(soup)
            
            return content, image_url
            
        except Exception as e:
            raise Exception(f"Smart request failed: {e}")

    async def method_simple_request(self, url):
        """–ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å"""
        headers = {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }
        
        await asyncio.sleep(random.uniform(1, 2))
        
        try:
            response = self.session.get(url, headers=headers, timeout=20, verify=False)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –í–°–ï–ú–ò —Å–ø–æ—Å–æ–±–∞–º–∏
            image_url = self.extract_all_possible_images(soup, url)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            content = self.extract_content(soup)
            
            return content, image_url
            
        except Exception as e:
            raise Exception(f"Simple request failed: {e}")

    def extract_all_possible_images(self, soup, base_url):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –í–°–ï–ú–ò –≤–æ–∑–º–æ–∂–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏"""
        logger.info("üîç –ù–∞—á–∏–Ω–∞—é –ü–û–õ–ù–´–ô –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ...")
        
        found_images = []
        
        # 1. –ú–µ—Ç–∞-—Ç–µ–≥–∏ (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π)
        meta_selectors = [
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
            'meta[itemprop="image"]',
            'meta[name="og:image:url"]',
            'meta[property="twitter:image:src"]',
            'link[rel="image_src"]',
            'link[rel="apple-touch-icon"]',
            'link[rel="apple-touch-startup-image"]',
        ]
        
        for selector in meta_selectors:
            elements = soup.select(selector)
            for element in elements:
                image_url = element.get('content') or element.get('href') or element.get('src')
                if image_url:
                    normalized_url = self.normalize_image_url(image_url, base_url)
                    if normalized_url and normalized_url not in found_images:
                        found_images.append(normalized_url)
                        logger.info(f"‚úÖ –ú–µ—Ç–∞-—Ç–µ–≥ {selector}: {normalized_url}")
        
        # 2. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç–∞—Ç—å–∏ iXBT
        article_selectors = [
            'div.b-article img',
            'article img',
            '.b-article__text img',
            '.article-content img',
            '.post-content img',
            '.entry-content img',
            '.article-body img',
            'figure img',
            '.b-article__image img',
            '.article-image',
            '.wp-block-image img',
            '.content img',
            'main img',
            '.news-img',
            '.post-thumbnail img',
            '.entry-thumbnail img',
        ]
        
        for selector in article_selectors:
            elements = soup.select(selector)
            for element in elements:
                # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                for attr in ['src', 'data-src', 'data-lazy-src', 'data-original', 'data-srcset', 'srcset']:
                    image_url = element.get(attr)
                    if image_url:
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º srcset
                        if attr == 'srcset' and ',' in image_url:
                            image_url = image_url.split(',')[0].split(' ')[0]
                        
                        normalized_url = self.normalize_image_url(image_url, base_url)
                        if normalized_url and normalized_url not in found_images:
                            found_images.append(normalized_url)
                            logger.info(f"‚úÖ –°—Ç–∞—Ç—å—è {selector} [{attr}]: {normalized_url}")
        
        # 3. –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        all_images = soup.find_all('img')
        for img in all_images:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∫–æ–Ω–∫–∏, –ª–æ–≥–æ—Ç–∏–ø—ã –∏ –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            src = img.get('src', '')
            if any(ignore in src.lower() for ignore in ['logo', 'icon', 'avatar', 'spacer', 'pixel', 'emoji', 'favicon']):
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            width = img.get('width')
            height = img.get('height')
            if width and height:
                try:
                    if int(width) < 100 or int(height) < 100:
                        continue
                except:
                    pass
            
            # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –∞—Ç—Ä–∏–±—É—Ç—ã
            for attr in ['src', 'data-src', 'data-lazy-src', 'data-original']:
                image_url = img.get(attr)
                if image_url:
                    normalized_url = self.normalize_image_url(image_url, base_url)
                    if normalized_url and normalized_url not in found_images and self.is_valid_image_url(normalized_url):
                        found_images.append(normalized_url)
                        logger.info(f"‚úÖ –û–±—â–∏–π –ø–æ–∏—Å–∫ [{attr}]: {normalized_url}")
        
        # 4. –ò—â–µ–º –≤ —Å—Ç–∏–ª—è—Ö (background-image)
        styles = soup.find_all(style=re.compile(r'background-image'))
        for style in styles:
            style_content = style.get('style', '')
            urls = re.findall(r'url\([\'"]?(.*?)[\'"]?\)', style_content)
            for image_url in urls:
                normalized_url = self.normalize_image_url(image_url, base_url)
                if normalized_url and normalized_url not in found_images:
                    found_images.append(normalized_url)
                    logger.info(f"‚úÖ CSS background: {normalized_url}")
        
        # 5. –ò—â–µ–º –≤ JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        script_tags = soup.find_all('script', type='application/ld+json')
        for script in script_tags:
            try:
                data = json.loads(script.string)
                images = self.extract_images_from_jsonld(data)
                for image_url in images:
                    normalized_url = self.normalize_image_url(image_url, base_url)
                    if normalized_url and normalized_url not in found_images:
                        found_images.append(normalized_url)
                        logger.info(f"‚úÖ JSON-LD: {normalized_url}")
            except:
                pass
        
        logger.info(f"üéØ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(found_images)}")
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ —Ä–∞–∑–º–µ—Ä—É –∏ –∫–∞—á–µ—Å—Ç–≤—É)
        if found_images:
            best_image = self.select_best_image(found_images)
            logger.info(f"üèÜ –í—ã–±—Ä–∞–Ω–æ –ª—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {best_image}")
            return best_image
        
        logger.warning("‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
        return ""

    def extract_images_from_jsonld(self, data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ JSON-LD –¥–∞–Ω–Ω—ã—Ö"""
        images = []
        
        if isinstance(data, dict):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
            for key in ['image', 'thumbnail', 'photo', 'logo']:
                if key in data:
                    image_data = data[key]
                    if isinstance(image_data, str):
                        images.append(image_data)
                    elif isinstance(image_data, dict) and 'url' in image_data:
                        images.append(image_data['url'])
                    elif isinstance(image_data, list):
                        for item in image_data:
                            if isinstance(item, str):
                                images.append(item)
                            elif isinstance(item, dict) and 'url' in item:
                                images.append(item['url'])
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            for value in data.values():
                if isinstance(value, (dict, list)):
                    images.extend(self.extract_images_from_jsonld(value))
        
        elif isinstance(data, list):
            for item in data:
                images.extend(self.extract_images_from_jsonld(item))
        
        return images

    def select_best_image(self, image_urls):
        """–í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö"""
        if not image_urls:
            return ""
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º
        extension_priority = ['.jpg', '.jpeg', '.png', '.webp', '.gif']
        
        for ext in extension_priority:
            for url in image_urls:
                if url.lower().endswith(ext):
                    return url
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ URL
        priority_keywords = ['large', 'big', 'main', 'featured', 'cover', 'hero']
        for keyword in priority_keywords:
            for url in image_urls:
                if keyword in url.lower():
                    return url
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        return image_urls[0]

    def extract_content(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in soup(["script", "style", "nav", "header", "footer", "aside", "form"]):
            element.decompose()
        
        # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –ø–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º iXBT
        selectors = [
            'div.b-article__text',
            'article.b-article',
            'div.b-article-body',
            'div.article-content',
            'div.post-content',
            'div.entry-content',
            'div.content',
            'article',
            'div.article__text',
            'div.article-body',
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                text = self.clean_and_extract_text(element)
                if len(text) > 200:
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É: {selector}")
                    return text
        
        # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Ç–µ–≥–∞–º
        semantic_tags = ['main', 'article', 'div[role="main"]']
        for tag in semantic_tags:
            if tag.startswith('div'):
                element = soup.find('div', role='main')
            else:
                element = soup.find(tag)
                
            if element:
                text = self.clean_and_extract_text(element)
                if len(text) > 200:
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ: {tag}")
                    return text
        
        return ""

    def clean_and_extract_text(self, element):
        """–û—á–∏—Å—Ç–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        # –ö–ª–æ–Ω–∏—Ä—É–µ–º —ç–ª–µ–º–µ–Ω—Ç
        element = BeautifulSoup(str(element), 'html.parser')
        
        # –£–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        garbage_selectors = [
            'div.ad', 'div.adv', 'div.advertisement', 'div.banner',
            'div.comments', 'div.social', 'div.share', 'div.related',
            'div.recommended', 'div.teaser', 'div.meta', 'div.tags',
            'ins', 'iframe', 'a[href*="ad"]', 'div[class*="ad"]',
            'div[class*="banner"]', 'div.widget', 'div.subscribe',
            'div.navigation', 'div.pagination', 'div.author',
        ]
        
        for selector in garbage_selectors:
            for elem in element.select(selector):
                elem.decompose()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        paragraphs = element.find_all('p')
        text_parts = []
        
        for p in paragraphs:
            text = p.get_text(strip=True)
            if self.is_meaningful_text(text):
                text_parts.append(text)
        
        text = '\n'.join(text_parts)
        return self.post_process_text(text)

    def is_meaningful_text(self, text):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º"""
        if len(text) < 40:
            return False
            
        garbage_indicators = [
            '—Ä–µ–∫–ª–∞–º–∞', '–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å', '–∏—Å—Ç–æ—á–Ω–∏–∫:', '—á–∏—Ç–∞—Ç—å —Ç–∞–∫–∂–µ',
            '–∫–æ–º–º–µ–Ω—Ç–∞—Ä', '—Ñ–æ—Ç–æ:', '–≤–∏–¥–µ–æ:', '—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ', 'share',
            '—Ç–µ–≥–∏:', '–æ—Ü–µ–Ω–∏—Ç–µ —Å—Ç–∞—Ç—å—é', '–ø–æ–¥–µ–ª–∏—Ç—å—Å—è', '—Ä–µ–¥–∞–∫—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç',
        ]
        
        if any(indicator in text.lower() for indicator in garbage_indicators):
            return False
            
        return len(text.split()) >= 5

    def post_process_text(self, text):
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º URL
        text = re.sub(r'https?://\S+', '', text)
        
        # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
        text = re.sub(r'<[^>]+>', '', text)
        
        return text.strip()

    def is_valid_image_url(self, url):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ URL –ø–æ—Ö–æ–∂ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
        if not url or url.strip() == '':
            return False
        
        url_lower = url.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        valid_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp', '.tiff', '.svg']
        if any(url_lower.endswith(ext) for ext in valid_extensions):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ URL
        image_patterns = [
            '/images/', '/img/', '/uploads/', '/media/',
            'image', 'photo', 'picture', 'img', 'upload'
        ]
        if any(pattern in url_lower for pattern in image_patterns):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        if any(param in url_lower for param in ['/wp-content/', '/content/images/']):
            return True
        
        return False

    def normalize_image_url(self, image_url, base_url):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not image_url:
            return ""
        
        # –û—á–∏—â–∞–µ–º URL –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –∫–æ–¥–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        image_url = image_url.strip()
        image_url = image_url.replace(' ', '%20')  # –ö–æ–¥–∏—Ä—É–µ–º –ø—Ä–æ–±–µ–ª—ã
        
        # –£–¥–∞–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∏ —è–∫–æ—Ä—è
        image_url = image_url.split('?')[0].split('#')[0]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL
        if image_url.startswith('//'):
            image_url = 'https:' + image_url
        elif image_url.startswith('/'):
            image_url = 'https://www.ixbt.com' + image_url
        elif not image_url.startswith(('http://', 'https://')):
            # –ï—Å–ª–∏ URL –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –±–µ–∑ —Å–ª–µ—à–∞
            if image_url.startswith('./'):
                image_url = image_url[2:]
            base_domain = 'https://www.ixbt.com'
            image_url = base_domain + '/' + image_url.lstrip('/')
        
        return image_url

    async def alternative_content_fetch(self, url):
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ RSS"""
        try:
            headers = {
                'User-Agent': self.ua.random,
                'Accept': 'application/rss+xml,application/xml;q=0.9,*/*;q=0.8',
            }
            
            response = self.session.get(IXBT_RSS_URL, headers=headers, timeout=20)
            feed = feedparser.parse(response.content)
            
            for entry in feed.entries:
                if entry.link == url and hasattr(entry, 'summary'):
                    soup = BeautifulSoup(entry.summary, 'html.parser')
                    text = soup.get_text(strip=True)
                    
                    # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ RSS –æ–ø–∏—Å–∞–Ω–∏–∏
                    image_url = ""
                    img_tag = soup.find('img')
                    if img_tag and img_tag.get('src'):
                        image_url = self.normalize_image_url(img_tag.get('src'), url)
                    
                    if len(text) > 100:
                        logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ RSS –æ–ø–∏—Å–∞–Ω–∏–µ")
                        return text, image_url
            
            return "", ""
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è: {e}")
            return "", ""

    async def download_image(self, image_url, filename):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            if not image_url:
                logger.warning("‚ùå URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—É—Å—Ç–æ–π")
                return False
                
            logger.info(f"üñºÔ∏è –°–∫–∞—á–∏–≤–∞—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
                'Referer': 'https://www.ixbt.com/',
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∞–π–º–∞—É—Ç—ã –∏ stream –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
            response = self.session.get(
                image_url, 
                headers=headers, 
                timeout=30, 
                verify=False,
                stream=True
            )
            response.raise_for_status()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º content-type
            content_type = response.headers.get('content-type', '').lower()
            logger.info(f"üìã Content-Type: {content_type}")
            
            if 'image' not in content_type:
                logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π content-type: {content_type}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            with open(filename, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            file_size = os.path.getsize(filename)
            logger.info(f"üì¶ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size} –±–∞–π—Ç")
            
            if file_size < 1024:  # –ú–∏–Ω–∏–º—É–º 1KB
                logger.warning(f"‚ö†Ô∏è –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π: {file_size} –±–∞–π—Ç")
                os.remove(filename)
                return False
            
            # –ü—Ä–æ–±—É–µ–º –æ—Ç–∫—Ä—ã—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
            try:
                with Image.open(filename) as img:
                    img.verify()  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–∞
                logger.info(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–∫–∞—á–∞–Ω–æ –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ: {filename}")
                return True
            except Exception as img_error:
                logger.error(f"‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {img_error}")
                os.remove(filename)
                return False
                
        except requests.exceptions.Timeout:
            logger.error("‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return False
        except requests.exceptions.ConnectionError:
            logger.error("‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return False
        except requests.exceptions.HTTPError as e:
            logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ {e.response.status_code}: {e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
            return False

    def create_news_image(self, title, filename):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Ç–æ–ª—å–∫–æ –∫–∞–∫ fallback)"""
        try:
            width, height = 1200, 630
            
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.new('RGB', (width, height), color=(255, 255, 255))
            draw = ImageDraw.Draw(image)
            
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —à—Ä–∏—Ñ—Ç—ã
            try:
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —à—Ä–∏—Ñ—Ç—ã
                font_sizes = [48, 42, 36, 32]
                font = None
                
                for size in font_sizes:
                    try:
                        font = ImageFont.truetype("arial.ttf", size)
                        break
                    except:
                        try:
                            font = ImageFont.truetype("DejaVuSans.ttf", size)
                            break
                        except:
                            continue
                
                if not font:
                    font = ImageFont.load_default()
            except:
                font = ImageFont.load_default()
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ —Å—Ç—Ä–æ–∫–∏
            words = title.split()
            lines = []
            current_line = []
            
            for word in words:
                test_line = ' '.join(current_line + [word])
                bbox = draw.textbbox((0, 0), test_line, font=font)
                text_width = bbox[2] - bbox[0]
                
                if text_width < width - 100:
                    current_line.append(word)
                else:
                    lines.append(' '.join(current_line))
                    current_line = [word]
            
            if current_line:
                lines.append(' '.join(current_line))
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
            if len(lines) > 4:
                lines = lines[:4]
                lines[-1] = lines[-1][:50] + '...'
            
            # –†–∏—Å—É–µ–º —Ç–µ–∫—Å—Ç
            y = (height - len(lines) * 50) // 2
            
            for line in lines:
                bbox = draw.textbbox((0, 0), line, font=font)
                text_width = bbox[2] - bbox[0]
                x = (width - text_width) // 2
                
                draw.text((x, y), line, fill=(0, 0, 0), font=font)
                y += 55
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image.save(filename, 'JPEG', quality=85)
            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –∑–∞–≥–ª—É—à–µ—á–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {filename}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return False

    def format_news_message(self, news_item):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        title = news_item['title']
        text = news_item['full_text']
        link = news_item['link']
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        banned_orgs = self.check_banned_organizations(title, text)
        if banned_orgs:
            logger.warning(f"üö´ –ù–æ–≤–æ—Å—Ç—å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞ –∏–∑-–∑–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π: {banned_orgs}")
            return None
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        categories = self.detect_news_category(title, text)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ö–µ—à—Ç–µ–≥–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        hashtags = self.get_hashtags_for_category(categories)
        
        # –û–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ 800 —Å–∏–º–≤–æ–ª–æ–≤
        if len(text) > 800:
            text = text[:797] + "..."
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
        message = f"üì∞ {title}\n\n"
        message += f"{text}\n\n"
        message += f"üîó –ß–∏—Ç–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ: {link}\n\n"
        message += " ".join(hashtags)
        
        return message

    async def process_and_send_news(self, news_item):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏"""
        try:
            message = self.format_news_message(news_item)
            if not message:
                return False
            
            image_path = None
            image_downloaded = False
            
            # –ü—Ä–æ–±—É–µ–º —Å–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if news_item['image_url']:
                image_filename = f"downloaded_images/{news_item['hash']}.jpg"
                image_downloaded = await self.download_image(news_item['image_url'], image_filename)
                
                if image_downloaded:
                    image_path = image_filename
                    logger.info(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ –∫ –æ—Ç–ø—Ä–∞–≤–∫–µ: {image_path}")
                else:
                    logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É")
            
            # –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ —Å–∫–∞—á–∞–Ω–æ, —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
            if not image_downloaded:
                image_filename = f"downloaded_images/{news_item['hash']}_fallback.jpg"
                if self.create_news_image(news_item['title'], image_filename):
                    image_path = image_filename
                    logger.info(f"‚úÖ –ó–∞–≥–ª—É—à–∫–∞ —Å–æ–∑–¥–∞–Ω–∞: {image_path}")
                else:
                    logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∑–∞–≥–ª—É—à–∫—É")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            if image_path and os.path.exists(image_path):
                with open(image_path, 'rb') as photo:
                    await self.bot.send_photo(
                        chat_id=CHANNEL_ID,
                        photo=photo,
                        caption=message,
                        parse_mode='HTML'
                    )
                logger.info(f"‚úÖ –ù–æ–≤–æ—Å—Ç—å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º: {news_item['title']}")
            else:
                await self.bot.send_message(
                    chat_id=CHANNEL_ID,
                    text=message,
                    parse_mode='HTML'
                )
                logger.info(f"‚úÖ –ù–æ–≤–æ—Å—Ç—å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {news_item['title']}")
            
            # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é
            self.processed_news.add(news_item['hash'])
            self.save_processed_news()
            
            # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª—ã –ø–æ—Å–ª–µ –æ—Ç–ø—Ä–∞–≤–∫–∏
            if image_path and os.path.exists(image_path):
                try:
                    os.remove(image_path)
                    logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_path}")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
            return False

    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –±–æ—Ç–∞"""
        logger.info("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –∏ –Ω–∞—á–∞–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π...")
        
        while True:
            try:
                logger.info("üîç –ü—Ä–æ–≤–µ—Ä—è—é –Ω–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏...")
                news_list = await self.fetch_news()
                
                if news_list:
                    logger.info(f"üì• –ù–∞–π–¥–µ–Ω–æ {len(news_list)} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
                    
                    for news_item in news_list:
                        await self.process_and_send_news(news_item)
                        await asyncio.sleep(10)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∫–∞–º–∏
                else:
                    logger.info("‚ÑπÔ∏è –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                
                logger.info(f"‚è∞ –û–∂–∏–¥–∞—é {CHECK_INTERVAL} —Å–µ–∫—É–Ω–¥ –¥–æ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏...")
                await asyncio.sleep(CHECK_INTERVAL)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
                await asyncio.sleep(60)  # –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    bot = SmartNewsBot()
    await bot.run()

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞...")
    logger.info(f"üì° –ö–∞–Ω–∞–ª: {CHANNEL_ID}")
    logger.info(f"‚è∞ –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–∫–∏: {CHECK_INTERVAL} —Å–µ–∫—É–Ω–¥")
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

        sys.exit(1)
