import asyncio
import logging
import hashlib
import json
import os
import sys
import re
import random
import requests
import time
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import feedparser
from PIL import Image, ImageDraw, ImageFont
from telegram import Bot
from telegram.error import TelegramError
from fake_useragent import UserAgent
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import io
import banned_organizations
import news_tags

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è - —Ç–µ–ø–µ—Ä—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –±–ª–æ–∫
if 'RAILWAY_ENVIRONMENT' in os.environ or 'RAILWAY_STATIC_URL' in os.environ:
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è Railway
    TOKEN = os.environ.get('TELEGRAM_TOKEN', "7445394461:AAGHiGYBiCwEg-tbchU9lOJmywv4CjcKuls")
    CHANNEL_ID = os.environ.get('TELEGRAM_CHANNEL', "@techno_met")
else:
    # –õ–æ–∫–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    TOKEN = "7445394461:AAGHiGYBiCwEg-tbchU9lOJmywv4CjcKuls"
    CHANNEL_ID = "@techno_met"

IXBT_RSS_URL = "https://www.ixbt.com/export/news.rss"
CHECK_INTERVAL = 1800  # 30 –º–∏–Ω—É—Ç

# –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –æ–±–ª–∞–∫–µ
def ensure_directories():
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
    directories = ['images', 'downloaded_images']
    for directory in directories:
        os.makedirs(directory, exist_ok=True)

# –í—ã–∑–æ–≤ –≤ –Ω–∞—á–∞–ª–µ
ensure_directories()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ httpx
logging.getLogger('httpx').setLevel(logging.WARNING)

class SmartNewsBot:
    def __init__(self):
        self.processed_news = set()
        self.load_processed_news()
        self.bot = Bot(token=TOKEN)
        self.ua = UserAgent()
        self.session = self.create_advanced_session()
        
    def create_advanced_session(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Å–µ—Å—Å–∏–∏ —Å –æ–±—Ö–æ–¥–æ–º –∑–∞—â–∏—Ç—ã"""
        session = requests.Session()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º retry —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"],
            backoff_factor=1
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        return session

    def load_processed_news(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists('processed_news.json'):
                with open('processed_news.json', 'r') as f:
                    data = json.load(f)
                    self.processed_news = set(data)
        except Exception as e:
            logger.error(f"Error loading processed news: {e}")
            self.processed_news = set()

    def save_processed_news(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Ñ–∞–π–ª"""
        try:
            with open('processed_news.json', 'w') as f:
                json.dump(list(self.processed_news), f)
        except Exception as e:
            logger.error(f"Error saving processed news: {e}")

    def get_news_hash(self, title, link):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ö—ç—à–∞ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏"""
        return hashlib.md5(f"{title}{link}".encode()).hexdigest()

    def check_banned_organizations(self, title, text):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        content = f"{title} {text}".lower()
        
        found_organizations = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (—Ç–æ–ª—å–∫–æ —Ü–µ–ª—ã–µ —Å–ª–æ–≤–∞)
        for org in banned_organizations.BANNED_ORGANIZATIONS:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            pattern = r'\b' + re.escape(org.lower()) + r'\b'
            if re.search(pattern, content):
                found_organizations.append(org)
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in banned_organizations.BANNED_KEYWORDS:
            # –ò—â–µ–º —Ç–æ–ª—å–∫–æ —Ü–µ–ª—ã–µ —Å–ª–æ–≤–∞
            pattern = r'\b' + re.escape(keyword) + r'\b'
            if re.search(pattern, content):
                # –ü–æ–ª—É—á–∞–µ–º –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
                matches = re.finditer(pattern, content)
                for match in matches:
                    start = max(0, match.start() - 30)
                    end = min(len(content), match.end() + 30)
                    context = content[start:end]
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è
                    if len(keyword) > 2:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —á–∞—Å—Ç—å –¥—Ä—É–≥–æ–≥–æ —Å–ª–æ–≤–∞
                        words_in_context = re.findall(r'\b\w+\b', context)
                        if any(keyword == word.lower() for word in words_in_context):
                            found_organizations.append(f"–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: '{keyword}' –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ: ...{context}...")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (–º–µ–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤)
        # –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —á–∞—Å—Ç—å—é –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        filtered_organizations = []
        for org in found_organizations:
            if "–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ:" in org:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–∑ —Å—Ç—Ä–æ–∫–∏
                match = re.search(r"–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: '([^']*)'", org)
                if match and len(match.group(1)) < 3:
                    logger.info(f"üîç –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: '{match.group(1)}'")
                    continue
            filtered_organizations.append(org)
        
        return filtered_organizations

    def format_news_message(self, news_item):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –ë–ï–ó –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏ —Ö–µ—à—Ç–µ–≥–æ–≤"""
        title = news_item['title']
        text = news_item['full_text']
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        banned_orgs = self.check_banned_organizations(title, text)
        if banned_orgs:
            logger.warning(f"üö´ –ù–æ–≤–æ—Å—Ç—å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞ –∏–∑-–∑–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π: {banned_orgs}")
            return None
        
        # –û–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ 800 —Å–∏–º–≤–æ–ª–æ–≤
        if len(text) > 800:
            text = text[:797] + "..."
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ë–ï–ó —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫ –∏ —Ö–µ—à—Ç–µ–≥–æ–≤
        message = f"üì∞ {title}\n\n"
        message += f"{text}"
        
        return message

    async def fetch_news(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å IXBT"""
        try:
            headers = {
                'User-Agent': self.ua.random,
                'Accept': 'application/rss+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            }
            
            response = self.session.get(IXBT_RSS_URL, headers=headers, timeout=20)
            response.raise_for_status()
            
            feed = feedparser.parse(response.content)
            news_list = []
            
            for entry in feed.entries[:5]:
                news_hash = self.get_news_hash(entry.title, entry.link)
                
                if news_hash not in self.processed_news:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    await asyncio.sleep(random.uniform(2, 5))
                    
                    full_text, image_url = await self.get_full_article_text_and_image(entry.link)
                    
                    if len(full_text) > 150:
                        news_item = {
                            'title': entry.title,
                            'link': entry.link,
                            'summary': entry.summary,
                            'full_text': full_text,
                            'image_url': image_url,
                            'hash': news_hash,
                            'published': entry.published
                        }
                        news_list.append(news_item)
                        logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {entry.title}")
                        if image_url:
                            logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏: {image_url}")
                        else:
                            logger.warning("‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏")
                    else:
                        logger.warning(f"‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç: {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return news_list
        except Exception as e:
            logger.error(f"Error fetching news: {e}")
            return []

    async def get_full_article_text_and_image(self, url):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Å—Ç–∞—Ç—å–∏"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏ —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            methods = [
                self.method_smart_request,
                self.method_simple_request,
            ]
            
            best_result = None
            all_found_images = []  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–ï –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
            for method in methods:
                try:
                    full_text, image_url = await method(url)
                    logger.info(f"üîÑ –ú–µ—Ç–æ–¥ {method.__name__}: —Ç–µ–∫—Å—Ç={len(full_text) if full_text else 0}, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ={image_url}")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    if image_url and image_url not in all_found_images:
                        all_found_images.append(image_url)
                        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ –º–µ—Ç–æ–¥–∞ {method.__name__}: {image_url}")
                    
                    if full_text and len(full_text) > 150:
                        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π –º–µ—Ç–æ–¥: {method.__name__}")
                        if not best_result or len(full_text) > len(best_result):
                            best_result = full_text
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ú–µ—Ç–æ–¥ {method.__name__} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
                    continue
            
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ —Å –ª—É—á—à–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
            if best_result:
                best_image = self.select_best_image(all_found_images) if all_found_images else ""
                logger.info(f"‚úÖ –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º: {best_image}")
                return best_result, best_image
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç, –Ω–æ –Ω–∞—à–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è - –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if not best_result and all_found_images:
                logger.info("üñºÔ∏è –ù–∞–π–¥–µ–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω–æ –Ω–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç. –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
                image_only = await self.extract_image_only(url)
                if image_only and image_only not in all_found_images:
                    all_found_images.append(image_only)
            
            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö
            best_image = self.select_best_image(all_found_images) if all_found_images else ""
            
            # Fallback - –∏—Å–ø–æ–ª—å–∑—É–µ–º RSS –æ–ø–∏—Å–∞–Ω–∏–µ
            rss_text, rss_image = await self.alternative_content_fetch(url)
            if rss_text and len(rss_text) > 100:
                logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ RSS –æ–ø–∏—Å–∞–Ω–∏–µ")
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ RSS –µ—Å–ª–∏ –µ—Å—Ç—å
                if rss_image and rss_image not in all_found_images:
                    all_found_images.append(rss_image)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                best_image = self.select_best_image(all_found_images) if all_found_images else ""
                logger.info(f"üñºÔ∏è –§–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {best_image}")
                return rss_text, best_image
            
            logger.info(f"üñºÔ∏è –§–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {best_image}")
            return "", best_image
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {e}")
            return "", ""

    async def method_smart_request(self, url):
        """–£–º–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π JavaScript –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            headers = {
                'User-Agent': self.ua.random,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0',
            }
            
            response = self.session.get(url, headers=headers, timeout=30, verify=False)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for element in soup.find_all(['script', 'style', 'nav', 'footer', 'aside']):
                element.decompose()
            
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content_selectors = [
                'article',
                '.article-content',
                '.post-content',
                '.entry-content',
                '.content',
                '.news-text',
                '[class*="content"]',
                '[class*="article"]',
                '[class*="post"]',
                '[class*="entry"]',
                'main'
            ]
            
            content = None
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    break
            
            if not content:
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º body
                content = soup.find('body')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text = content.get_text(separator='\n', strip=True) if content else ""
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            cleaned_text = '\n'.join(lines)
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_url = ""
            image_selectors = [
                'meta[property="og:image"]',
                'meta[name="twitter:image"]',
                'img[class*="article"]',
                'img[class*="news"]',
                'img[class*="post"]',
                'img[class*="entry"]',
                '.article-image img',
                '.post-image img',
                '.news-image img',
                '.entry-image img',
                'figure img'
            ]
            
            for selector in image_selectors:
                img_tag = soup.select_one(selector)
                if img_tag:
                    if img_tag.get('src'):
                        image_url = img_tag['src']
                        break
                    elif img_tag.get('content'):
                        image_url = img_tag['content']
                        break
            
            # –î–µ–ª–∞–µ–º URL –∞–±—Å–æ–ª—é—Ç–Ω—ã–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if image_url and image_url.startswith('//'):
                image_url = 'https:' + image_url
            elif image_url and image_url.startswith('/'):
                from urllib.parse import urljoin
                image_url = urljoin(url, image_url)
            
            return cleaned_text, image_url
            
        except Exception as e:
            logger.error(f"Error in smart request: {e}")
            return "", ""

    async def method_simple_request(self, url):
        """–ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            headers = {
                'User-Agent': self.ua.random,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            }
            
            response = self.session.get(url, headers=headers, timeout=15, verify=False)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for element in soup.find_all(['script', 'style']):
                element.decompose()
            
            # –ò—â–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            content_areas = soup.find_all(['article', 'div', 'section'], 
                                        class_=lambda x: x and any(word in x for word in 
                                                                  ['content', 'article', 'post', 'entry', 'news']))
            
            if not content_areas:
                content_areas = [soup.find('body')]
            
            text_parts = []
            for content in content_areas:
                if content:
                    text = content.get_text(separator='\n', strip=True)
                    lines = [line.strip() for line in text.split('\n') if line.strip()]
                    text_parts.extend(lines)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –æ—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            cleaned_text = '\n'.join(text_parts)
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_url = ""
            img_tags = soup.find_all('img')
            for img in img_tags:
                src = img.get('src') or img.get('data-src')
                if src:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
                    width = img.get('width')
                    height = img.get('height')
                    
                    # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –±–æ–ª—å—à–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    if width and height:
                        try:
                            if int(width) >= 300 and int(height) >= 200:
                                image_url = src
                                break
                        except:
                            continue
                    else:
                        # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–∞–∑–º–µ—Ä–æ–≤, –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ
                        if not image_url and ('article' in str(img.parent) or 'news' in str(img.parent)):
                            image_url = src
            
            # –î–µ–ª–∞–µ–º URL –∞–±—Å–æ–ª—é—Ç–Ω—ã–º
            if image_url and image_url.startswith('//'):
                image_url = 'https:' + image_url
            elif image_url and image_url.startswith('/'):
                from urllib.parse import urljoin
                image_url = urljoin(url, image_url)
            
            return cleaned_text, image_url
            
        except Exception as e:
            logger.error(f"Error in simple request: {e}")
            return "", ""

    async def extract_image_only(self, url):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–æ–≥–¥–∞ —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"""
        try:
            headers = {'User-Agent': self.ua.random}
            response = self.session.get(url, headers=headers, timeout=15, verify=False)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º Open Graph –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                image_url = og_image['content']
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url
                elif image_url.startswith('/'):
                    from urllib.parse import urljoin
                    image_url = urljoin(url, image_url)
                return image_url
            
            # –ò—â–µ–º Twitter –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
            if twitter_image and twitter_image.get('content'):
                image_url = twitter_image['content']
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url
                elif image_url.startswith('/'):
                    from urllib.parse import urljoin
                    image_url = urljoin(url, image_url)
                return image_url
            
            return ""
            
        except Exception as e:
            logger.error(f"Error extracting image only: {e}")
            return ""

    async def alternative_content_fetch(self, url):
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ RSS –æ–ø–∏—Å–∞–Ω–∏–µ"""
        try:
            headers = {'User-Agent': self.ua.random}
            response = self.session.get(url, headers=headers, timeout=10, verify=False)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ meta
            description = soup.find('meta', attrs={'name': 'description'})
            if description and description.get('content'):
                text = description['content']
                
                # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_url = ""
                og_image = soup.find('meta', property='og:image')
                if og_image and og_image.get('content'):
                    image_url = og_image['content']
                    if image_url.startswith('//'):
                        image_url = 'https:' + image_url
                    elif image_url.startswith('/'):
                        from urllib.parse import urljoin
                        image_url = urljoin(url, image_url)
                
                return text, image_url
            
            return "", ""
            
        except Exception as e:
            logger.error(f"Error in alternative content fetch: {e}")
            return "", ""

    def select_best_image(self, image_urls):
        """–í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Å–ø–∏—Å–∫–∞"""
        if not image_urls:
            return ""
        
        # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –≤ URL
        preferred_keywords = ['og:', 'twitter:', 'cover', 'featured', 'main', 'article', 'news']
        
        for url in image_urls:
            if any(keyword in url.lower() for keyword in preferred_keywords):
                return url
        
        # –ò–Ω–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        return image_urls[0]

    async def download_image(self, image_url):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            if not image_url:
                return None
                
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ URL
            filename = hashlib.md5(image_url.encode()).hexdigest() + '.jpg'
            filepath = os.path.join('downloaded_images', filename)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–∫–∞—á–∞–Ω–æ –ª–∏ —É–∂–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if os.path.exists(filepath):
                logger.info(f"üñºÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {filename}")
                return filepath
            
            headers = {
                'User-Agent': self.ua.random,
                'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                'Referer': 'https://www.ixbt.com/'
            }
            
            response = self.session.get(image_url, headers=headers, timeout=15, stream=True, verify=False)
            response.raise_for_status()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            logger.info(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–∫–∞—á–∞–Ω–æ: {filename}")
            return filepath
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_url}: {e}")
            return None

    async def create_news_image(self, title, image_path=None):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º"""
        try:
            # –†–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è Telegram
            width, height = 1200, 630
            
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if image_path and os.path.exists(image_path):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–∞—á–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–∫ —Ñ–æ–Ω
                try:
                    background = Image.open(image_path)
                    background = background.resize((width, height), Image.Resampling.LANCZOS)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è —Ñ–æ–Ω–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
                    background = Image.new('RGB', (width, height), color=(25, 25, 35))
            else:
                # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Ñ–æ–Ω
                background = Image.new('RGB', (width, height), color=(25, 25, 35))
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–ª—É–ø—Ä–æ–∑—Ä–∞—á–Ω—ã–π overlay –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
            overlay = Image.new('RGBA', (width, height), (0, 0, 0, 180))
            background = background.convert('RGBA')
            background = Image.alpha_composite(background, overlay)
            
            draw = ImageDraw.Draw(background)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —à—Ä–∏—Ñ—Ç—ã
            try:
                title_font = ImageFont.truetype("arialbd.ttf", 48)
            except:
                try:
                    title_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 48)
                except:
                    title_font = ImageFont.load_default()
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ —Å—Ç—Ä–æ–∫–∏
            words = title.split()
            lines = []
            current_line = []
            
            for word in words:
                test_line = ' '.join(current_line + [word])
                bbox = draw.textbbox((0, 0), test_line, font=title_font)
                text_width = bbox[2] - bbox[0]
                
                if text_width < width - 100:  # 100px padding
                    current_line.append(word)
                else:
                    lines.append(' '.join(current_line))
                    current_line = [word]
            
            if current_line:
                lines.append(' '.join(current_line))
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
            if len(lines) > 3:
                lines = lines[:3]
                lines[-1] = lines[-1][:97] + '...'
            
            # –†–∏—Å—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            total_text_height = len(lines) * 60
            y_position = (height - total_text_height) // 2
            
            for line in lines:
                bbox = draw.textbbox((0, 0), line, font=title_font)
                text_width = bbox[2] - bbox[0]
                x_position = (width - text_width) // 2
                
                # –¢–µ–Ω—å —Ç–µ–∫—Å—Ç–∞
                draw.text((x_position+2, y_position+2), line, font=title_font, fill=(0, 0, 0, 160))
                # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
                draw.text((x_position, y_position), line, font=title_font, fill=(255, 255, 255))
                
                y_position += 60
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            output_path = os.path.join('images', f"news_{int(time.time())}.jpg")
            background.convert('RGB').save(output_path, 'JPEG', quality=85)
            
            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏: {e}")
            return None

    async def send_telegram_message(self, message, image_path=None):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Telegram –∫–∞–Ω–∞–ª"""
        try:
            if image_path and os.path.exists(image_path):
                with open(image_path, 'rb') as photo:
                    await self.bot.send_photo(
                        chat_id=CHANNEL_ID,
                        photo=photo,
                        caption=message,
                        parse_mode='HTML'
                    )
                logger.info("‚úÖ –ù–æ–≤–æ—Å—Ç—å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º")
            else:
                await self.bot.send_message(
                    chat_id=CHANNEL_ID,
                    text=message,
                    parse_mode='HTML',
                    disable_web_page_preview=False
                )
                logger.info("‚úÖ –ù–æ–≤–æ—Å—Ç—å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return True
        except TelegramError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {e}")
            return False

    async def process_and_send_news(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π...")
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏
            news_list = await self.fetch_news()
            
            if not news_list:
                logger.info("üì≠ –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                return
            
            logger.info(f"üì® –ù–∞–π–¥–µ–Ω–æ {len(news_list)} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –Ω–æ–≤–æ—Å—Ç—å
            for news_item in news_list:
                try:
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                    message = self.format_news_message(news_item)
                    
                    if not message:
                        logger.warning("‚ùå –°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ (–≤–æ–∑–º–æ–∂–Ω–æ, –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ)")
                        continue
                    
                    # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    image_path = None
                    if news_item['image_url']:
                        image_path = await self.download_image(news_item['image_url'])
                    
                    # –ï—Å–ª–∏ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Å—Ç–∞—Ç—å–∏, —Å–æ–∑–¥–∞–µ–º —Å–≤–æ–µ
                    if not image_path:
                        logger.info("üé® –°–æ–∑–¥–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º")
                        image_path = await self.create_news_image(news_item['title'])
                    
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                    success = await self.send_telegram_message(message, image_path)
                    
                    if success:
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
                        self.processed_news.add(news_item['hash'])
                        self.save_processed_news()
                        
                        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {news_item['title']}")
                        
                        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∫–∞–º–∏
                        await asyncio.sleep(10)
                    else:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {news_item['title']}")
                        
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                    continue
            
            logger.info("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ process_and_send_news: {e}")

    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –±–æ—Ç–∞"""
        logger.info("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
        
        while True:
            try:
                await self.process_and_send_news()
                logger.info(f"üí§ –û–∂–∏–¥–∞–Ω–∏–µ {CHECK_INTERVAL} —Å–µ–∫—É–Ω–¥...")
                await asyncio.sleep(CHECK_INTERVAL)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
                await asyncio.sleep(60)  # –ñ–¥–µ–º –º–∏–Ω—É—Ç—É –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    bot = SmartNewsBot()
    await bot.run()

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤
    required_files = ['banned_organizations.py', 'news_tags.py']
    for file in required_files:
        if not os.path.exists(file):
            logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π —Ñ–∞–π–ª: {file}")
            sys.exit(1)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)
